{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e94e4e-8ca0-462a-9f41-95a2f41a944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers # requires transformers==4.35.2\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1668d6c1-d608-4f91-8741-b33693e83d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c100275b-9db8-432f-bbec-b88c715ac61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "draft_model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, trust_remote_code=True, device_map=\"cuda:0\", torch_dtype=torch.float16, use_flash_attention_2=True)\n",
    "print(draft_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e27a50-63e6-45c1-88c2-e43d2fa94bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d73395578b24c97bd066b1c5e142712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"cuda:1\", torch_dtype=torch.float16, use_flash_attention_2=True) # , use_flash_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac464b9a-caae-41b6-88ce-fbd42fd2bf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "640f2d62-7976-4d26-beb1-08aa01043736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n",
    "from transformers.models.auto import (\n",
    "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
    "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
    ")\n",
    "from transformers.utils import ExplicitEnum, ModelOutput, is_accelerate_available, logging\n",
    "from transformers.generation.beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from transformers.generation.logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    EncoderRepetitionPenaltyLogitsProcessor,\n",
    "    EpsilonLogitsWarper,\n",
    "    EtaLogitsWarper,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    MinNewTokensLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SequenceBiasLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    "    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "\n",
    "from transformers.generation.utils import _crop_past_key_values\n",
    "import difflib\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using greedy search.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
    "            if all batches finished early due to the `eos_token_id`.\n",
    "        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
    "            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
    "        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5466bcc-865b-4333-9cae-4d3fa4afd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            # if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "            #     return input_ids[0, start_idx:end_idx]\n",
    "            if start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:min(end_idx, input_length)]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7bc8977-4805-48d6-b1af-d0be832876ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
    "UNDERLINE = \"\\x1b[4m\"\n",
    "RESET = \"\\x1b[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4401af4-c003-4a8a-ad08-a19c1a523a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_search_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        draft_matching_window_size = 3,\n",
    "        draft_num_candidate_tokens = 10,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            if input_ids[0, -1] == 185 and input_ids[0, -2] == 185 and input_ids[0, -3] == 185: # hacky stopping criteria for new-line ending models\n",
    "                break\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15854661-e128-4d81-b89a-1773d659e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def assistant_greedy_search_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        prompt_matching_window_size = 3,\n",
    "        prompt_num_candidate_tokens = 10,\n",
    "        draft_num_candidate_rounds = 4,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        # scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        scores = None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "        matching_original = True\n",
    "\n",
    "        input_token_len = input_ids.shape[-1]\n",
    "    \n",
    "        for i in range(draft_num_candidate_rounds):\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, prompt_matching_window_size, prompt_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # print(model_inputs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if input_ids[0, -1] == 185 and input_ids[0, -2] == 185 and input_ids[0, -3] == 185: # hacky stopping criteria for new-line ending models\n",
    "                break\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        return input_ids[0, input_token_len:], model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4700d120-8047-4f4e-8f5e-420d34909e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_search_assistant_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        assistant_model: torch.nn.Module,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        assistant_prompt_matching_window_size = 3,\n",
    "        assistant_prompt_candidate_tokens = 10,\n",
    "        assistant_draft_candidate_rounds = 4,\n",
    "        max_draft_num_candidate_tokens = 300,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "\n",
    "        assistant_model_kwargs = {}\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            \n",
    "            input_ids = input_ids.to(assistant_model.device)\n",
    "            candidate_pred_tokens, assistant_model_kwargs = assistant_model.assistant_greedy_search_pld(input_ids,\n",
    "                  stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=cur_len + max_draft_num_candidate_tokens)]),\n",
    "                  draft_num_candidate_rounds=assistant_draft_candidate_rounds,\n",
    "                  prompt_matching_window_size=assistant_prompt_matching_window_size,\n",
    "                  prompt_num_candidate_tokens = assistant_prompt_candidate_tokens,\n",
    "                  use_cache=True, \n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  eos_token_id=tokenizer.eos_token_id,\n",
    "                    print_output=False\n",
    "            )\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            candidate_pred_tokens = candidate_pred_tokens.to(self.device)\n",
    "\n",
    "            # print(candidate_pred_tokens)\n",
    "            \n",
    "            # candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if input_ids[0, -1] == 185 and input_ids[0, -2] == 185 and input_ids[0, -3] == 185: # hacky stopping criteria for new-line ending models\n",
    "                break\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "            # New cache size - 1 because the target model generates another token not yet considered by the drafter/assistant\n",
    "            if \"past_key_values\" in assistant_model_kwargs:\n",
    "                assistant_model_kwargs[\"past_key_values\"] = _crop_past_key_values(assistant_model, assistant_model_kwargs[\"past_key_values\"], new_cache_size - 1) \n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd3b3c4-7eb8-47f2-9bec-87d0a12bf98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1202]\n",
      "[32013, 185]\n",
      "[32013, 1672]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"...\"))\n",
    "print(tokenizer.encode(\"\"\"\n",
    "\"\"\"))\n",
    "print(tokenizer.encode(\"##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "575a21f7-2aed-47ab-9f27-7c52a3c506fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the average\n",
    "average_throughput = np.mean(tokens_per_sec_arr)\n",
    "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Throughput Values')\n",
    "plt.xlabel('Tokens per Second')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "question = \"Can you please change x axis to start from 0\"\n",
    "prompt = \"[INST] Code:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:[/INST]\".format(code_text=code_text, question=question)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Move all tensor values in the inputs to GPU\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9dcce41-44b2-4260-b844-8348b785d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.greedy_search_assistant_pld = greedy_search_assistant_pld.__get__(model, type(model))\n",
    "model.greedy_search_pld = greedy_search_pld.__get__(model, type(model))\n",
    "# draft_model.greedy_search_pld = greedy_search_pld.__get__(draft_model, type(draft_model))\n",
    "draft_model.assistant_greedy_search_pld = assistant_greedy_search_pld.__get__(draft_model, type(draft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "039203dd-acd3-4e72-855b-f1f780a9d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device:  cuda:1\n",
      "Draft model device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Model device: \", model.device)\n",
    "print(\"Draft model device: \", draft_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebaeb3f8-710f-4813-81f4-df9386f21790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nuprl/CanItEdit\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da1d739d-f4fc-4b71-85b1-508a075ebb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import StoppingCriteriaList, MaxLengthCriteria\n",
    "\n",
    "# Define the variable for max_new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9881114-6ac1-4d97-9a1d-a5693321de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 105/105 [14:08<00:00,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with_assistant': [0.47544099390506744, 2.090827189385891, 2.428807495161891, 1.631228832527995, 1.8104624561965466, 4.784645726904273, 1.2146416753530502, 1.9357067998498678, 3.3488631322979927, 2.3714559208601713, 7.771988749504089, 1.3990313913673162, 2.0762216839939356, 2.494663033634424, 2.204437520354986, 4.210124753415585, 0.7794880103319883, 1.2868676520884037, 5.368718264624476, 0.9373744539916515, 2.6756424587219954, 3.936274666339159, 2.3919489569962025, 2.292264051735401, 1.120786802843213, 9.154633603990078, 1.2756009176373482, 1.4592125751078129, 1.2113991361111403, 0.6803446393460035, 0.5225058272480965, 1.4784732591360807, 8.813761200755835, 1.4497700221836567, 1.2881670221686363, 1.0995314866304398, 3.486190464347601, 1.2929817363619804, 0.437933711335063, 1.8921039383858442, 2.6846073046326637, 3.921631835401058, 9.504176566377282, 2.228879787027836, 10.925382168963552, 4.892832903191447, 3.1977977994829416, 3.0805010609328747, 0.9550783075392246, 1.532419653609395, 6.576894789934158, 0.9298096224665642, 0.5165425278246403, 0.5529611892998219, 1.591248732060194, 1.4637591615319252, 3.0548480041325092, 4.140170203521848, 1.689096612855792, 0.8086230419576168, 0.5797733254730701, 0.22897870652377605, 4.684257505461574, 0.6437781285494566, 0.8549559321254492, 0.38179355673491955, 2.5662841014564037, 1.0141759738326073, 0.6778041981160641, 0.6025543678551912, 2.6443333327770233, 2.7366736344993114, 0.7203794792294502, 0.8808930944651365, 0.8291695918887854, 0.3244825955480337, 0.7279114238917828, 0.7356863413006067, 0.6844278797507286, 1.9083096720278263, 13.478314043954015, 2.4353806525468826, 1.15899957716465, 1.1067831572145224, 10.819804150611162, 5.946566363796592, 3.0194863099604845, 1.0715237390249968, 1.9134502075612545, 3.0553710367530584, 1.8066586516797543, 1.4659505598247051, 2.681783178821206, 2.094533173367381, 1.5277136601507664, 4.24987218901515, 8.076367173343897, 3.3672539480030537, 2.7733392268419266, 0.3799546957015991, 0.98259636759758, 1.1305240653455257, 2.5101240165531635, 0.6454578377306461, 1.3633558135479689], 'without_assistant': [1.0812866110354662, 5.667312886565924, 7.433900684118271, 4.734918812289834, 5.29210663959384, 10.835686026141047, 3.254180520772934, 4.277568783611059, 1.0088122505694628, 6.38592872954905, 16.184703826904297, 4.496636537835002, 10.631722711026669, 0.49542354978621006, 6.350409545004368, 0.6316327732056379, 1.3254762068390846, 2.645297324284911, 13.355768859386444, 3.5347550474107265, 7.900903584435582, 10.07317310757935, 10.044159324839711, 0.7378189116716385, 2.4471493754535913, 17.748475996777415, 1.1492519732564688, 3.165093159303069, 2.5463274642825127, 3.0751960519701242, 5.62239988707006, 3.934353983029723, 18.753231374546885, 5.03808824531734, 5.369804698973894, 4.634787380695343, 8.879846515133977, 4.468180378898978, 1.4966448806226254, 9.322663931176066, 5.569725017994642, 9.653350526466966, 28.751748928800225, 6.247533692047, 1.6505041979253292, 12.2211819794029, 0.5120224487036467, 8.416359532624483, 1.2249414157122374, 4.166013415902853, 7.034262182191014, 2.155043888837099, 1.9873093403875828, 1.838233008980751, 6.867987243458629, 4.09982599876821, 8.688818167895079, 12.688342919573188, 2.3269679322838783, 2.782880673184991, 3.166642054915428, 0.6237452477216721, 9.126935375854373, 1.7109477519989014, 2.7657836508005857, 2.31720657274127, 0.6845156066119671, 3.121692191809416, 3.195833323523402, 2.410952692851424, 12.044449044391513, 7.246661677956581, 2.5076719857752323, 2.986176962032914, 2.963737029582262, 1.0432281196117401, 2.518133368343115, 3.2319493032991886, 2.3656303510069847, 5.89762094989419, 22.336464231833816, 0.23215274885296822, 2.3967766519635916, 2.895322360098362, 0.5819448083639145, 2.1109877806156874, 8.044624017551541, 0.34292590990662575, 2.0397933833301067, 13.873042847961187, 5.238342160359025, 3.5339053496718407, 7.787388430908322, 6.728334655985236, 4.232085261493921, 10.904004322364926, 18.212758934125304, 1.0335529800504446, 11.548223488032818, 1.2949695494025946, 9.309635072946548, 4.143189126625657, 0.31045343540608883, 1.8446914423257113, 4.4091802053153515]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "time_taken = {\"with_assistant\": [], \"without_assistant\": []}\n",
    "outputs = []\n",
    "\n",
    "for row in tqdm(ds):\n",
    "    input_text = f\"# Code Before:\\n{row['before']}\\n# Instruction:\\n{row['instruction_descriptive']}\\n# Code After:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "    max_new_tokens = inputs['input_ids'].shape[-1] + 300\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    test_out = model.greedy_search_assistant_pld(inputs.input_ids,\n",
    "                    draft_model,\n",
    "                  attention_mask = inputs.attention_mask,\n",
    "                  stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                assistant_prompt_matching_window_size = 3,\n",
    "                assistant_prompt_candidate_tokens = 50,\n",
    "                assistant_draft_candidate_rounds = 4,\n",
    "                max_draft_num_candidate_tokens = 300,\n",
    "                  use_cache=True, \n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  eos_token_id=tokenizer.eos_token_id,\n",
    "                print_output=False\n",
    "            )\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    time_taken[\"with_assistant\"].append(end_time - start_time)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    test_out = model.greedy_search_pld(inputs.input_ids,\n",
    "                    draft_model,\n",
    "                  attention_mask = inputs.attention_mask,\n",
    "                  stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                prompt_matching_window_size = 3,\n",
    "                prompt_num_candidate_tokens = 50,\n",
    "                  use_cache=True, \n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  eos_token_id=tokenizer.eos_token_id,\n",
    "                 print_output=False\n",
    "            )\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    time_taken[\"without_assistant\"].append(end_time - start_time)\n",
    "\n",
    "    outputs.append(tokenizer.batch_decode(test_out))\n",
    "\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46a1571f-9dea-4ee9-8ad8-659d843c63d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266.31557012349367"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_taken['with_assistant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fef3a28-dbe6-43af-b0ae-bfe5af3c0325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582.228393478319"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_taken['without_assistant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "310f2040-4f62-4030-83a2-06a637889756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4396993258334843, 0.36892743196552463, 0.32672046592588694, 0.34451041236314744, 0.3421061931464587, 0.44156371044355985, 0.3732557759471033, 0.4525249967379305, 3.3196098980832147, 0.3713564653308673, 0.48020580621218967, 0.31112841333645097, 0.19528553748308228, 5.0354147167831345, 0.34713312656962975, 6.665462800558186, 0.5880814806859975, 0.4864737284064186, 0.40197747663559913, 0.26518795260743616, 0.33865018477037084, 0.3907680950481622, 0.23814327109296168, 3.1068111910305145, 0.457996889803864, 0.5157982919577029, 1.1099401587476614, 0.4610330570583019, 0.4757436555602169, 0.22123618392074249, 0.09293288235326574, 0.3757855204471344, 0.46998626661847986, 0.28776193500206115, 0.23989085160113732, 0.23723450426446108, 0.3925958020114498, 0.28937545638669787, 0.2926103025541212, 0.20295743280613493, 0.4819999723432031, 0.4062456682422303, 0.330559945758885, 0.35676154733909804, 6.619421012498284, 0.40035676675444615, 6.245424995679817, 0.3660134823128544, 0.7796930492254588, 0.367838386635942, 0.9349800475997617, 0.4314573950363054, 0.2599205454969077, 0.3008112609219347, 0.2316906941805649, 0.35702958173632504, 0.35158383396951276, 0.32629715556750694, 0.7258787667082173, 0.29057050478278407, 0.18308773628933384, 0.36710292761372837, 0.5132344333075854, 0.376269893570584, 0.30911887554110495, 0.16476457525461588, 3.749051265840837, 0.3248801968668038, 0.21208997137835267, 0.249923762354106, 0.21954788658501195, 0.3776461157037209, 0.28727021847984996, 0.29499025197269174, 0.27977164762342516, 0.3110370487988734, 0.28906785996435724, 0.22762929497366025, 0.2893215668540041, 0.32357279117133897, 0.60342200556276, 10.490423501680388, 0.48356594938252756, 0.38226595161477006, 18.592491925531682, 2.816959159309878, 0.3753421295231016, 3.124650859180511, 0.9380607973330181, 0.22023798745796294, 0.3448913027009923, 0.4148245113471513, 0.34437516538627344, 0.3113003856762943, 0.36098366780339514, 0.38975334779520815, 0.4434455648677797, 3.2579403407445144, 0.2401528884261618, 0.2934082086153167, 0.1055461744631611, 0.27286325359379865, 8.085347850217197, 0.3499001637460189, 0.3092084582763066]\n"
     ]
    }
   ],
   "source": [
    "ratios = []\n",
    "for i, j in zip(time_taken['with_assistant'], time_taken['without_assistant']):\n",
    "    ratios.append(i / j)\n",
    "print(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5567176-474d-46f5-9491-66505503ad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ratios, bins=80)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
