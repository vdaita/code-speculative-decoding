{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e94e4e-8ca0-462a-9f41-95a2f41a944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers # requires transformers==4.35.2\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1668d6c1-d608-4f91-8741-b33693e83d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c100275b-9db8-432f-bbec-b88c715ac61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "draft_model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, trust_remote_code=True, device_map=\"cuda:0\", torch_dtype=torch.float16, use_flash_attention_2=True)\n",
    "print(draft_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e27a50-63e6-45c1-88c2-e43d2fa94bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bd1fef897c43a4baeec4507cc47b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372bd0a171db4fd5935b164bdab8ccf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d30440e38747d194faf3bd3fa13dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f64bbd1c550478d924fa7d09ce241a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/750 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7616e8dfccbe4808a6633d676b1373ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3978848fd661441f9618f436e08650bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763d478e1359435eaa2297e366273445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2a5d287c894ca7881cd5e4430a467b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bb6849b243431ca220bf1fee0b053d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13c1537efed4a4ab08b1ce9b773ba87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed5f21d555c47d6b87cffc20b2c6cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25e9370b7f14e30bcc151e744fa7d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f66c87c2b3547619caed70586ef1bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f7bb61e675490ab796255b7151c8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"nuprl/EditCoder-6.7b-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"cuda:1\", torch_dtype=torch.float16, use_flash_attention_2=True) # , use_flash_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac464b9a-caae-41b6-88ce-fbd42fd2bf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be64b3a4-aff3-4330-8e09-e2ff43823861",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "640f2d62-7976-4d26-beb1-08aa01043736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n",
    "from transformers.models.auto import (\n",
    "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
    "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
    ")\n",
    "from transformers.utils import ExplicitEnum, ModelOutput, is_accelerate_available, logging\n",
    "from transformers.generation.beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from transformers.generation.logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    EncoderRepetitionPenaltyLogitsProcessor,\n",
    "    EpsilonLogitsWarper,\n",
    "    EtaLogitsWarper,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    MinNewTokensLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SequenceBiasLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    "    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "\n",
    "from transformers.generation.utils import _crop_past_key_values\n",
    "import difflib\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using greedy search.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
    "            if all batches finished early due to the `eos_token_id`.\n",
    "        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
    "            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
    "        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5466bcc-865b-4333-9cae-4d3fa4afd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            # if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "            #     return input_ids[0, start_idx:end_idx]\n",
    "            if start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:min(end_idx, input_length)]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bc8977-4805-48d6-b1af-d0be832876ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
    "UNDERLINE = \"\\x1b[4m\"\n",
    "RESET = \"\\x1b[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4401af4-c003-4a8a-ad08-a19c1a523a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_search_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        draft_matching_window_size = 3,\n",
    "        draft_num_candidate_tokens = 10,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            if input_ids.shape[-1] > NEWLINE_THRESHOLD: # Check that there are max 5 consecutive newlines.\n",
    "                flag = True\n",
    "                for i in range(NEWLINE_THRESHOLD):\n",
    "                    if not(input_ids[0, -i] == 185): # Is a newline\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    break\n",
    "\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15854661-e128-4d81-b89a-1773d659e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def assistant_greedy_search_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        prompt_matching_window_size = 3,\n",
    "        prompt_num_candidate_tokens = 10,\n",
    "        draft_num_candidate_rounds = 4,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        # scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        scores = None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "        matching_original = True\n",
    "\n",
    "        input_token_len = input_ids.shape[-1]\n",
    "    \n",
    "        for i in range(draft_num_candidate_rounds):\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, prompt_matching_window_size, prompt_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # print(model_inputs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if input_ids.shape[-1] > NEWLINE_THRESHOLD: # Check that there are max 5 consecutive newlines.\n",
    "                flag = True\n",
    "                for i in range(NEWLINE_THRESHOLD):\n",
    "                    if not(input_ids[0, -i] == 185): # Is a newline\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    break\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        return input_ids[0, input_token_len:], model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4700d120-8047-4f4e-8f5e-420d34909e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_search_assistant_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        assistant_model: torch.nn.Module,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        assistant_prompt_matching_window_size = 3,\n",
    "        assistant_prompt_candidate_tokens = 10,\n",
    "        assistant_draft_candidate_rounds = 4,\n",
    "        max_draft_num_candidate_tokens = 300,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "\n",
    "        assistant_model_kwargs = {}\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            \n",
    "            input_ids = input_ids.to(assistant_model.device)\n",
    "            candidate_pred_tokens, assistant_model_kwargs = assistant_model.assistant_greedy_search_pld(input_ids,\n",
    "                  stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=cur_len + max_draft_num_candidate_tokens)]),\n",
    "                  draft_num_candidate_rounds=assistant_draft_candidate_rounds,\n",
    "                  prompt_matching_window_size=assistant_prompt_matching_window_size,\n",
    "                  prompt_num_candidate_tokens = assistant_prompt_candidate_tokens,\n",
    "                  use_cache=True, \n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  eos_token_id=tokenizer.eos_token_id,\n",
    "                    print_output=False\n",
    "            )\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            candidate_pred_tokens = candidate_pred_tokens.to(self.device)\n",
    "\n",
    "            # print(candidate_pred_tokens)\n",
    "            \n",
    "            # candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if input_ids.shape[-1] > NEWLINE_THRESHOLD: # Check that there are max 5 consecutive newlines.\n",
    "                flag = True\n",
    "                for i in range(NEWLINE_THRESHOLD):\n",
    "                    if not(input_ids[0, -i] == 185): # Is a newline\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    break\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "            # New cache size - 1 because the target model generates another token not yet considered by the drafter/assistant\n",
    "            if \"past_key_values\" in assistant_model_kwargs:\n",
    "                assistant_model_kwargs[\"past_key_values\"] = _crop_past_key_values(assistant_model, assistant_model_kwargs[\"past_key_values\"], new_cache_size - 1) \n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd3b3c4-7eb8-47f2-9bec-87d0a12bf98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1202]\n",
      "[32013, 185]\n",
      "[32013, 1672]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"...\"))\n",
    "print(tokenizer.encode(\"\"\"\n",
    "\"\"\"))\n",
    "print(tokenizer.encode(\"##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "575a21f7-2aed-47ab-9f27-7c52a3c506fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the average\n",
    "average_throughput = np.mean(tokens_per_sec_arr)\n",
    "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Throughput Values')\n",
    "plt.xlabel('Tokens per Second')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "question = \"Can you please change x axis to start from 0\"\n",
    "prompt = \"[INST] Code:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:[/INST]\".format(code_text=code_text, question=question)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Move all tensor values in the inputs to GPU\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9dcce41-44b2-4260-b844-8348b785d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.greedy_search_assistant_pld = greedy_search_assistant_pld.__get__(model, type(model))\n",
    "model.greedy_search_pld = greedy_search_pld.__get__(model, type(model))\n",
    "# draft_model.greedy_search_pld = greedy_search_pld.__get__(draft_model, type(draft_model))\n",
    "draft_model.assistant_greedy_search_pld = assistant_greedy_search_pld.__get__(draft_model, type(draft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "039203dd-acd3-4e72-855b-f1f780a9d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device:  cuda:1\n",
      "Draft model device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Model device: \", model.device)\n",
    "print(\"Draft model device: \", draft_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebaeb3f8-710f-4813-81f4-df9386f21790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nuprl/CanItEdit\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da1d739d-f4fc-4b71-85b1-508a075ebb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import StoppingCriteriaList, MaxLengthCriteria\n",
    "\n",
    "# Define the variable for max_new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9881114-6ac1-4d97-9a1d-a5693321de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▍                                                                                                                                                 | 1/105 [00:01<02:55,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4538276884704828 1.230972858145833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▊                                                                                                                                                | 2/105 [00:06<05:39,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2540243994444609 3.171100275591016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▏                                                                                                                                              | 3/105 [00:17<11:36,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.750144274905324 8.284551467746496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▌                                                                                                                                             | 4/105 [00:23<11:23,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6720860451459885 4.989906162023544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███████                                                                                                                                            | 5/105 [00:29<10:36,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5538300853222609 4.084243148565292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████▍                                                                                                                                          | 6/105 [00:37<11:24,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8768189623951912 6.1026456244289875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████▊                                                                                                                                         | 7/105 [00:41<10:00,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2236054632812738 3.2781308963894844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████▏                                                                                                                                       | 8/105 [00:49<10:42,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9786671120673418 5.724685620516539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████████▌                                                                                                                                      | 9/105 [01:04<14:29,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3258551992475986 11.076128706336021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▉                                                                                                                                    | 10/105 [01:12<13:59,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.919160744175315 6.414564076811075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████████▎                                                                                                                                  | 11/105 [01:38<22:10, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.698372451588511 17.513250902295113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████████████▋                                                                                                                                 | 12/105 [01:49<20:33, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0632686279714108 8.151579240337014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████████████                                                                                                                                | 13/105 [02:02<20:04, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0285568721592426 10.653402142226696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████████████▍                                                                                                                              | 14/105 [02:13<19:01, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4427174776792526 8.83808403275907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████████████▊                                                                                                                             | 15/105 [02:22<17:09, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.307597864419222 6.558632433414459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████████████████▏                                                                                                                           | 16/105 [02:43<21:06, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.907233694568276 14.822290189564228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████████████████▋                                                                                                                          | 17/105 [02:47<16:12, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.274885406717658 2.3620735593140125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████████████████                                                                                                                         | 18/105 [02:51<12:58,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.293491082265973 2.7680901400744915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████████████████▍                                                                                                                       | 19/105 [03:02<13:40,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3316118344664574 8.570831036195159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████████████▊                                                                                                                      | 20/105 [03:05<11:01,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0749134365469217 2.63391600176692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████▏                                                                                                                    | 21/105 [03:16<12:08,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.700130892917514 8.038764402270317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██████████████████████████████▌                                                                                                                   | 22/105 [03:30<14:11, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.703210668638349 9.256520392373204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████████████████████▉                                                                                                                  | 23/105 [03:42<14:56, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5892247781157494 9.898142375051975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████████████████████████████▎                                                                                                                | 24/105 [03:51<13:46, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.477395247668028 6.013638926669955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████████████████████████████▊                                                                                                               | 25/105 [03:54<10:36,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6757347639650106 2.032932970672846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████████████████████▏                                                                                                             | 26/105 [04:01<10:19,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.878501195460558 5.718003157526255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████████████████████████████████▌                                                                                                            | 27/105 [04:09<10:17,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2470621448010206 5.836407892405987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████████████████████▉                                                                                                           | 28/105 [04:14<09:04,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6479701288044453 3.44662394374609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████████████████████████████▎                                                                                                         | 29/105 [04:18<07:43,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3468994982540607 2.4964146185666323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████████████████████████████▋                                                                                                        | 30/105 [04:19<05:37,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3502858839929104 0.38584453240036964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████████████████                                                                                                       | 31/105 [04:27<06:49,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2738414648920298 5.6856419295072556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████████▍                                                                                                     | 32/105 [04:32<06:42,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5241206772625446 3.9541401732712984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████████████████████████████▉                                                                                                    | 33/105 [04:51<11:17,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.749031033366919 13.722161596640944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████████████████████████████▎                                                                                                  | 34/105 [04:56<09:39,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.443529261276126 3.8282585199922323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████████████████████▋                                                                                                 | 35/105 [04:59<07:44,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9101830832660198 2.161925196647644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████████████████████████████                                                                                                | 36/105 [05:04<06:48,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1997607424855232 3.061540614813566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████████████████████████████▍                                                                                              | 37/105 [05:10<06:58,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.031111605465412 4.629427021369338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████████████████████████████▊                                                                                             | 38/105 [05:13<05:54,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.089410139247775 2.1816601026803255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████████████████████████████▏                                                                                           | 39/105 [05:16<04:46,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5592460595071316 1.5795563124120235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████████████████████████████▌                                                                                          | 40/105 [05:18<04:03,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6582640260457993 1.693761020898819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|█████████████████████████████████████████████████████████                                                                                         | 41/105 [05:26<05:28,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.857315745204687 5.516958594322205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████▍                                                                                       | 42/105 [05:32<05:25,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1731609106063843 4.068315457552671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████████████████████████████████████████████▊                                                                                      | 43/105 [06:10<15:38, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.321406641975045 29.06429072842002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████████████████████████████████▏                                                                                    | 44/105 [06:19<13:38, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.567900763824582 6.836517468094826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████████████████████████████████████████████████▌                                                                                   | 45/105 [06:48<18:03, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.267993411049247 21.602573612704873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████████████████████████████████▉                                                                                  | 46/105 [06:53<13:55, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6087208911776543 3.441439136862755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████████████████████████████████▎                                                                                | 47/105 [07:07<13:33, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.288145534694195 10.439294653013349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████████████████████████████████▋                                                                               | 48/105 [07:09<09:48, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.522784287109971 1.134805504232645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████████████████████████▏                                                                             | 49/105 [07:13<07:49,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9725674707442522 2.8980202320963144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████████████████████████████████▌                                                                            | 50/105 [07:13<05:35,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30180431716144085 0.44754099287092686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|██████████████████████████████████████████████████████████████████████▉                                                                           | 51/105 [07:27<07:32,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.140773763880134 9.591903178021312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████████████████▎                                                                         | 52/105 [07:40<08:36,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6993280444294214 9.236754842102528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████▋                                                                        | 53/105 [07:44<07:02,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5433253832161427 2.7524166237562895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████████████████████████████████████████████████████████████                                                                       | 54/105 [07:49<05:56,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4414680190384388 2.916719414293766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████████████████████████████████████████████████████████████▍                                                                     | 55/105 [07:58<06:25,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.367196246981621 7.0148019809275866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████████████████████████████████████████████████████████████▊                                                                    | 56/105 [08:04<05:47,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4711959697306156 4.189928770065308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████████████████████████████████████████▎                                                                  | 57/105 [08:11<05:47,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.022877387702465 5.576710935682058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████████████████████████████████████████████▋                                                                 | 58/105 [08:18<05:30,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8689730167388916 4.662128152325749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████████████████████████████                                                                | 59/105 [08:32<07:07,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.646613100543618 9.901446804404259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████████████████████████████████████████████▍                                                              | 60/105 [08:36<05:43,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9036140143871307 2.8743583112955093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████████████████████████████████████████▊                                                             | 61/105 [08:38<04:16,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45877161622047424 1.124035645276308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████████████████████████████████████████████████████████████████████▏                                                           | 62/105 [08:39<03:06,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22129898332059383 0.6184846069663763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████████████████████████▌                                                          | 63/105 [08:40<02:27,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5290021970868111 1.0873383060097694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|████████████████████████████████████████████████████████████████████████████████████████▉                                                         | 64/105 [08:43<02:16,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9925550278276205 1.8734357040375471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████████████████████████████████████████████████████████████████████████▍                                                       | 65/105 [08:48<02:34,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3130298517644405 3.7895261086523533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                      | 66/105 [08:51<02:20,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49922975711524487 2.497772505506873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 67/105 [09:03<03:55,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2480910774320364 9.013808572664857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████████████████████████████████████████████████████████████████████████▌                                                   | 68/105 [09:08<03:32,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5139264222234488 3.205845082178712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████████████████████████████████████████████████████████████████████████▉                                                  | 69/105 [09:12<03:05,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6627626568078995 3.051202245056629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████████████████████████████████████████████████████████████████████▎                                                | 70/105 [09:15<02:37,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6048879642039537 2.4232599399983883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 71/105 [09:31<04:32,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4604434706270695 12.74199766293168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 72/105 [09:40<04:31,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.558050574734807 6.115751679986715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                            | 73/105 [09:43<03:34,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7305672727525234 2.4528877548873425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 74/105 [09:46<02:50,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7837170623242855 1.8918381091207266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                         | 75/105 [09:47<02:08,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3875006027519703 1.0187069438397884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                        | 76/105 [09:48<01:38,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32390259206295013 1.036571055650711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████                                       | 77/105 [09:49<01:14,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43448565900325775 0.5615168120712042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                     | 78/105 [09:51<00:59,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3170661684125662 0.7574953697621822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                    | 79/105 [09:56<01:20,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3948177583515644 3.813143713399768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                  | 80/105 [10:03<01:49,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8070019166916609 5.536198738962412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                 | 81/105 [10:12<02:20,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.433866959065199 6.830691462382674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                | 82/105 [10:23<02:45,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.215707838535309 7.111083056777716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 83/105 [10:26<02:13,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0754988584667444 2.4490077551454306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 84/105 [10:32<02:06,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5960426591336727 4.253885520622134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                           | 85/105 [10:59<04:04, 12.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.851507496088743 18.866840351372957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 86/105 [11:15<04:15, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.74868100322783 11.60313980281353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 87/105 [11:26<03:50, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.040218599140644 8.16019512526691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 88/105 [11:31<02:55, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1959790270775557 3.27922360599041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                      | 89/105 [11:40<02:37,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.911890672519803 6.8663518484681845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 90/105 [11:57<03:03, 12.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.484088659286499 14.322602292522788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                   | 91/105 [12:05<02:30, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8719700556248426 5.342082403600216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 92/105 [12:12<02:04,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4022791124880314 4.564309619367123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 93/105 [12:21<01:56,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.204092336818576 7.6557212844491005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 94/105 [12:33<01:53, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2352388203144073 8.575012715533376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████              | 95/105 [12:38<01:25,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0628324653953314 3.4588215313851833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 96/105 [12:46<01:14,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3547689095139503 5.388820374384522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 97/105 [13:17<02:01, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.96715484559536 20.286531204357743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎         | 98/105 [13:29<01:39, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9926429260522127 8.736973462626338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 99/105 [13:43<01:26, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1814154535531998 11.748900536447763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 100/105 [13:47<00:55, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0329229943454266 2.2736628763377666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 101/105 [14:00<00:46, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.590821435675025 9.394052343443036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 102/105 [14:07<00:30, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7219006437808275 5.214129848405719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 103/105 [14:13<00:18,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.266231667250395 4.109087221324444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 104/105 [14:16<00:07,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6499225273728371 1.8614243250340223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [14:22<00:00,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4147843681275845 4.481917062774301\n",
      "{'with_assistant': [0.4538276884704828, 1.2540243994444609, 2.750144274905324, 1.6720860451459885, 1.5538300853222609, 1.8768189623951912, 1.2236054632812738, 1.9786671120673418, 3.3258551992475986, 1.919160744175315, 8.698372451588511, 3.0632686279714108, 2.0285568721592426, 2.4427174776792526, 2.307597864419222, 5.907233694568276, 1.274885406717658, 1.293491082265973, 2.3316118344664574, 1.0749134365469217, 2.700130892917514, 4.703210668638349, 2.5892247781157494, 2.477395247668028, 0.6757347639650106, 1.878501195460558, 2.2470621448010206, 1.6479701288044453, 1.3468994982540607, 0.3502858839929104, 2.2738414648920298, 1.5241206772625446, 4.749031033366919, 1.443529261276126, 0.9101830832660198, 1.1997607424855232, 2.031111605465412, 1.089410139247775, 0.5592460595071316, 0.6582640260457993, 2.857315745204687, 1.1731609106063843, 9.321406641975045, 2.567900763824582, 7.267993411049247, 1.6087208911776543, 3.288145534694195, 0.522784287109971, 0.9725674707442522, 0.30180431716144085, 4.140773763880134, 3.6993280444294214, 1.5433253832161427, 1.4414680190384388, 2.367196246981621, 1.4711959697306156, 2.022877387702465, 1.8689730167388916, 4.646613100543618, 0.9036140143871307, 0.45877161622047424, 0.22129898332059383, 0.5290021970868111, 0.9925550278276205, 1.3130298517644405, 0.49922975711524487, 3.2480910774320364, 1.5139264222234488, 0.6627626568078995, 0.6048879642039537, 3.4604434706270695, 2.558050574734807, 0.7305672727525234, 0.7837170623242855, 0.3875006027519703, 0.32390259206295013, 0.43448565900325775, 0.3170661684125662, 1.3948177583515644, 1.8070019166916609, 2.433866959065199, 3.215707838535309, 1.0754988584667444, 1.5960426591336727, 7.851507496088743, 4.74868100322783, 3.040218599140644, 1.1959790270775557, 1.911890672519803, 3.484088659286499, 1.8719700556248426, 2.4022791124880314, 2.204092336818576, 3.2352388203144073, 1.0628324653953314, 2.3547689095139503, 10.96715484559536, 2.9926429260522127, 3.1814154535531998, 1.0329229943454266, 3.590821435675025, 1.7219006437808275, 2.266231667250395, 0.6499225273728371, 1.4147843681275845], 'without_assistant': [1.230972858145833, 3.171100275591016, 8.284551467746496, 4.989906162023544, 4.084243148565292, 6.1026456244289875, 3.2781308963894844, 5.724685620516539, 11.076128706336021, 6.414564076811075, 17.513250902295113, 8.151579240337014, 10.653402142226696, 8.83808403275907, 6.558632433414459, 14.822290189564228, 2.3620735593140125, 2.7680901400744915, 8.570831036195159, 2.63391600176692, 8.038764402270317, 9.256520392373204, 9.898142375051975, 6.013638926669955, 2.032932970672846, 5.718003157526255, 5.836407892405987, 3.44662394374609, 2.4964146185666323, 0.38584453240036964, 5.6856419295072556, 3.9541401732712984, 13.722161596640944, 3.8282585199922323, 2.161925196647644, 3.061540614813566, 4.629427021369338, 2.1816601026803255, 1.5795563124120235, 1.693761020898819, 5.516958594322205, 4.068315457552671, 29.06429072842002, 6.836517468094826, 21.602573612704873, 3.441439136862755, 10.439294653013349, 1.134805504232645, 2.8980202320963144, 0.44754099287092686, 9.591903178021312, 9.236754842102528, 2.7524166237562895, 2.916719414293766, 7.0148019809275866, 4.189928770065308, 5.576710935682058, 4.662128152325749, 9.901446804404259, 2.8743583112955093, 1.124035645276308, 0.6184846069663763, 1.0873383060097694, 1.8734357040375471, 3.7895261086523533, 2.497772505506873, 9.013808572664857, 3.205845082178712, 3.051202245056629, 2.4232599399983883, 12.74199766293168, 6.115751679986715, 2.4528877548873425, 1.8918381091207266, 1.0187069438397884, 1.036571055650711, 0.5615168120712042, 0.7574953697621822, 3.813143713399768, 5.536198738962412, 6.830691462382674, 7.111083056777716, 2.4490077551454306, 4.253885520622134, 18.866840351372957, 11.60313980281353, 8.16019512526691, 3.27922360599041, 6.8663518484681845, 14.322602292522788, 5.342082403600216, 4.564309619367123, 7.6557212844491005, 8.575012715533376, 3.4588215313851833, 5.388820374384522, 20.286531204357743, 8.736973462626338, 11.748900536447763, 2.2736628763377666, 9.394052343443036, 5.214129848405719, 4.109087221324444, 1.8614243250340223, 4.481917062774301]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "time_taken = {\"with_assistant\": [], \"without_assistant\": []}\n",
    "outputs = {\"with_assistant\": [], \"without_assistant\": []}\n",
    "\n",
    "for row in tqdm(ds):\n",
    "    input_text = \"## Code Before:\\n{code_text}\\n## Instruction: {question}\\n## Code After:\\n\".format(code_text=row['before'], question=row['instruction_descriptive'])\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "    max_new_tokens = inputs['input_ids'].shape[-1] + 300\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    test_out = model.greedy_search_assistant_pld(inputs.input_ids,\n",
    "                    draft_model,\n",
    "                  attention_mask = inputs.attention_mask,\n",
    "                  stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                assistant_prompt_matching_window_size = 3,\n",
    "                assistant_prompt_candidate_tokens = 50,\n",
    "                assistant_draft_candidate_rounds = 4,\n",
    "                max_draft_num_candidate_tokens = 300,\n",
    "                  use_cache=True, \n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  eos_token_id=tokenizer.eos_token_id,\n",
    "                print_output=False,\n",
    "                seed=10\n",
    "            )\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    time_taken[\"with_assistant\"].append(end_time - start_time)\n",
    "    outputs[\"with_assistant\"].append(tokenizer.batch_decode(test_out))\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    test_out = model.greedy_search_pld(inputs.input_ids,\n",
    "                    draft_model,\n",
    "                  attention_mask = inputs.attention_mask,\n",
    "                  stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                prompt_matching_window_size = 3,\n",
    "                prompt_num_candidate_tokens = 50,\n",
    "                  use_cache=True, \n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  eos_token_id=tokenizer.eos_token_id,\n",
    "                 print_output=False,\n",
    "                seed=10\n",
    "            )\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    time_taken[\"without_assistant\"].append(end_time - start_time)\n",
    "    outputs[\"without_assistant\"].append(tokenizer.batch_decode(test_out))\n",
    "\n",
    "    print(time_taken[\"with_assistant\"][-1], time_taken[\"without_assistant\"][-1])\n",
    "\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46a1571f-9dea-4ee9-8ad8-659d843c63d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233.2163159046322"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_taken['with_assistant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fef3a28-dbe6-43af-b0ae-bfe5af3c0325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "628.4626537952572"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_taken['without_assistant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "310f2040-4f62-4030-83a2-06a637889756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3686740007850912, 0.3954540350228254, 0.33196055158957183, 0.335093685302473, 0.3804450491318295, 0.3075418560898006, 0.3732631496286333, 0.34563768968833025, 0.3002723503334759, 0.29918802294191177, 0.4966737757664734, 0.37578836415074396, 0.19041399593081057, 0.27638540984959226, 0.35184131567773713, 0.39853717738756184, 0.5397314582734278, 0.4672864743599586, 0.27204034528506205, 0.40810467601314293, 0.3358887955660126, 0.5080970461117874, 0.2615869402567725, 0.41196275298155993, 0.3323940207144954, 0.328523987082449, 0.3850077284222671, 0.47814039352761195, 0.5395335727634101, 0.907842031125215, 0.3999269551413857, 0.3854493291778336, 0.34608476222357243, 0.3770720430027424, 0.42100581679578053, 0.3918813739332291, 0.43873930749741674, 0.49934915980237105, 0.3540526254826258, 0.38864043860005815, 0.5179150244385199, 0.2883652762050337, 0.32071681119196443, 0.37561532985305085, 0.336441089906751, 0.4674558599470621, 0.31497774935829187, 0.46068183945184293, 0.33559719838143953, 0.674361280796646, 0.43169469989732767, 0.4005008368921219, 0.5607164881564802, 0.4942086688127544, 0.337457315747151, 0.3511267256478168, 0.36273664011510354, 0.4008840932024864, 0.4692862762719444, 0.31437069304691545, 0.4081468573958791, 0.35780839301086226, 0.4865111384037436, 0.5298046928904521, 0.3464891952496365, 0.19986998656386287, 0.3603461346275036, 0.4722394200017223, 0.21721361076004284, 0.24961744888348877, 0.27157778255555654, 0.41827247223032504, 0.29783966726438216, 0.41426222389004275, 0.3803847662914452, 0.3124750496333502, 0.773771416390222, 0.41857175775491534, 0.365792076876105, 0.3263975882900344, 0.35631340874767314, 0.45221069882883075, 0.43915698356082894, 0.37519642956923066, 0.4161538100637705, 0.409258276977441, 0.37256689974569723, 0.36471408198354294, 0.2784434463471788, 0.24325807476378725, 0.35041953945960413, 0.5263181757641424, 0.28790132959721254, 0.3772867665203423, 0.30728167260185, 0.4369729821961077, 0.5406126229820655, 0.3425262694059532, 0.2707840996426623, 0.45429909820631625, 0.3822441375027451, 0.3302373922098083, 0.5515170511566657, 0.34915334383038005, 0.31566500412924525]\n"
     ]
    }
   ],
   "source": [
    "ratios = []\n",
    "assisted_sum = 0\n",
    "non_assisted_sum = 0\n",
    "for idx, (i, j) in enumerate(zip(time_taken['with_assistant'], time_taken['without_assistant'])):\n",
    "    ratios.append(i / j)\n",
    "    if i / j > 1:\n",
    "        print(outputs['with_assistant'][idx][0])\n",
    "        if not(outputs['with_assistant'][idx][0] == outputs['without_assistant'][idx][0]):\n",
    "            print(\"ERROR - with assistant and without assistant have different results. Without assistant:\\n\")\n",
    "            print(outputs[\"without_assistant\"][idx][0])\n",
    "        print(\"============\")\n",
    "    else:\n",
    "        assisted_sum += i\n",
    "        non_assisted_sum += j\n",
    "print(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11a4f8d5-e8b7-4eab-86fb-2c779fd5a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -11,8 +11,8 @@\n",
      "\n",
      "         return normalized_matrix.tolist()\n",
      " ## Instruction: Edit the code to include a method called `translate_from_list(self, adj_list: Dict[int, List[int]]) -> List[List[float]]` that creates the transition matrix that represents the adjacency list, assume all edges are undirected. All columns must sum to 1.\n",
      " ## Code After:\n",
      "+from typing import Dict, List\n",
      " import numpy as np\n",
      "-from typing import Dict, List\n",
      " \n",
      " class MarkovChain:\n",
      " \n",
      "@@ -25,30 +25,25 @@\n",
      "\n",
      " \n",
      "     def translate_from_list(self, adj_list: Dict[int, List[int]]) -> List[List[float]]:\n",
      "         \"\"\"\n",
      "-        Translates a dictionary of lists into a transition matrix.\n",
      "+        Translates an adjacency list into a transition matrix.\n",
      "         \"\"\"\n",
      "-        # Create a list of all nodes in the graph\n",
      "-        nodes = []\n",
      "-        for key in adj_list.keys():\n",
      "-            nodes.append(key)\n",
      "-            for value in adj_list[key]:\n",
      "-                if value not in nodes:\n",
      "-                    nodes.append(value)\n",
      "-        \n",
      "-        # Create a matrix of zeros\n",
      "+        # Create a list of lists to represent the transition matrix\n",
      "         matrix = []\n",
      "-        for i in range(len(nodes)):\n",
      "-            matrix.append([])\n",
      "-            for j in range(len(nodes)):\n",
      "-                matrix[i].append(0)\n",
      "-        \n",
      "-        # Fill in the matrix\n",
      "-        for key in adj_list.keys():\n",
      "-            for value in adj_list[key]:\n",
      "-                matrix[key][value] = 1\n",
      "-        \n",
      "-        # Normalize the matrix\n",
      "-        matrix = np.array(matrix)\n",
      "-        column_sums = np.sum(matrix, axis=0)\n",
      "-        normalized_matrix = matrix / column_sums\n",
      "-        return normalized_matrix.tolist()<｜end▁of▁sentence｜>\n",
      "+        # For each key in the adjacency list\n",
      "+        for key in adj_list:\n",
      "+            # Create a list to represent the row in the transition matrix\n",
      "+            row = []\n",
      "+            # For each value in the list of values\n",
      "+            for value in adj_list:\n",
      "+                # If the value is in the list of values for the key\n",
      "+                if value in adj_list[key]:\n",
      "+                    # Add 1/outdegree to the row\n",
      "+                    row.append(1/len(adj_list[key]))\n",
      "+                else:\n",
      "+                    # Add 0 to the row\n",
      "+                    row.append(0)\n",
      "+            # Add the row to the transition matrix\n",
      "+            matrix.append(row)\n",
      "+        # Return the transition matrix\n",
      "+        return matrix\n",
      "+<｜end▁of▁sentence｜>\n",
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -110,8 +110,5 @@\n",
      "\n",
      "         self.edges.append(edge)\n",
      " \n",
      "     def fibonacci(self, x: Node) -> dict:\n",
      "-        '''\n",
      "-        Returns a dictionary with all nodes in the graph as keys and the distance from x to each key as the value.\n",
      "-        If a node is unreachable from x, it should have None as its value.\n",
      "-        '''\n",
      "+        '''Returns a dictionary with all nodes in Graph.nodes as keys and the distance from x to each key as the value. If a node is unreachable from x, it should have None as its value. Distance is defined as smallest path. A path is defined as the sum of the weights of a set of edges which can be used to get from one node to another.'''\n",
      "         pass<｜end▁of▁sentence｜>\n",
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -44,10 +44,12 @@\n",
      "\n",
      "         if len(self.vals) != len(other.vals):\n",
      "             raise ValueError(\"Vectors must have the same length\")\n",
      "         if self.magnitude() == 0 or other.magnitude() == 0:\n",
      "-            raise ValueError(\"Vectors must have non-zero magnitude\")\n",
      "-        dot_product = sum(x * y for x, y in zip(self.vals, other.vals))\n",
      "-        return acos(dot_product / (self.magnitude() * other.magnitude()))\n",
      "+            raise ValueError(\"Vectors must not have magnitude of zero\")\n",
      "+        return self.dot_product(other) / (self.magnitude() * other.magnitude())\n",
      " \n",
      "     def magnitude(self) -> float:\n",
      "         return sqrt(sum(x ** 2 for x in self.vals))\n",
      "+\n",
      "+    def dot_product(self, other: Vector) -> float:\n",
      "+        return sum(x * y for x, y in zip(self.vals, other.vals))\n",
      " <｜end▁of▁sentence｜>\n",
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -12,7 +12,7 @@\n",
      "\n",
      " def reverseString(originalString):\n",
      "     reversedString = \"\"\n",
      "     for i in range(0, len(originalString)):\n",
      "-        reversedString += originalString[len(originalString) - 1 - i]\n",
      "+        reversedString += originalString[len(originalString) - i - 1]\n",
      "     return reversedString\n",
      " \n",
      " def isPalindrome(originalString):\n",
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -236,7 +236,7 @@\n",
      "\n",
      "             s = (i, j)\n",
      "             if col != \"TERM\":\n",
      "                 max_a: List[Action] = []\n",
      "-                max_a_val = -float(\"inf\")\n",
      "+                max_a_val = -1000000\n",
      "                 for a in actions:\n",
      "                     s_next = next_state(s, a)\n",
      "                     i_next, j_next = s_next\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "count = 0\n",
    "for wa, woa in zip(outputs['with_assistant'], outputs['without_assistant']):\n",
    "    if not(wa[0] == woa[0]):\n",
    "        count += 1\n",
    "        print(\"Discrepancy: \")\n",
    "        print(\"\\n\".join(difflib.unified_diff(woa[0].splitlines(), wa[0].splitlines())))\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38371787-d786-44db-b054-b5a4861058b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assisted_sum, non_assisted_sum, assisted_sum/non_assisted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5567176-474d-46f5-9491-66505503ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ratios, bins=80)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
