{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e94e4e-8ca0-462a-9f41-95a2f41a944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers # requires transformers==4.35.2\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1668d6c1-d608-4f91-8741-b33693e83d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c100275b-9db8-432f-bbec-b88c715ac61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "draft_model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, trust_remote_code=True, device_map=\"cuda:0\", torch_dtype=torch.float16, use_flash_attention_2=True)\n",
    "print(draft_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e27a50-63e6-45c1-88c2-e43d2fa94bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae17488f80d439bb45b97c210b49b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"cuda:1\", torch_dtype=torch.float16, use_flash_attention_2=True) # , use_flash_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac464b9a-caae-41b6-88ce-fbd42fd2bf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be64b3a4-aff3-4330-8e09-e2ff43823861",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "640f2d62-7976-4d26-beb1-08aa01043736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n",
    "from transformers.models.auto import (\n",
    "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
    "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
    ")\n",
    "from transformers.utils import ExplicitEnum, ModelOutput, is_accelerate_available, logging\n",
    "from transformers.generation.beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from transformers.generation.logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    EncoderRepetitionPenaltyLogitsProcessor,\n",
    "    EpsilonLogitsWarper,\n",
    "    EtaLogitsWarper,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    MinNewTokensLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SequenceBiasLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    "    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "\n",
    "from transformers.generation.utils import _crop_past_key_values\n",
    "import difflib\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using greedy search.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
    "            if all batches finished early due to the `eos_token_id`.\n",
    "        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
    "            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
    "        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5466bcc-865b-4333-9cae-4d3fa4afd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            # if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "            #     return input_ids[0, start_idx:end_idx]\n",
    "            if start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:min(end_idx, input_length)]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bc8977-4805-48d6-b1af-d0be832876ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
    "UNDERLINE = \"\\x1b[4m\"\n",
    "RESET = \"\\x1b[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4401af4-c003-4a8a-ad08-a19c1a523a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_search_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        draft_matching_window_size = 3,\n",
    "        draft_num_candidate_tokens = 10,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            if input_ids.shape[-1] > NEWLINE_THRESHOLD: # Check that there are max 5 consecutive newlines.\n",
    "                flag = True\n",
    "                for i in range(NEWLINE_THRESHOLD):\n",
    "                    if not(input_ids[0, -i] == 185): # Is a newline\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    break\n",
    "\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15854661-e128-4d81-b89a-1773d659e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def assistant_greedy_search_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        prompt_matching_window_size = 3,\n",
    "        prompt_num_candidate_tokens = 10,\n",
    "        draft_num_candidate_rounds = 4,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        # scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        scores = None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "        matching_original = True\n",
    "\n",
    "        input_token_len = input_ids.shape[-1]\n",
    "    \n",
    "        for i in range(draft_num_candidate_rounds):\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, prompt_matching_window_size, prompt_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # print(model_inputs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if input_ids.shape[-1] > NEWLINE_THRESHOLD: # Check that there are max 5 consecutive newlines.\n",
    "                flag = True\n",
    "                for i in range(NEWLINE_THRESHOLD):\n",
    "                    if not(input_ids[0, -i] == 185): # Is a newline\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    break\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        return input_ids[0, input_token_len:], model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4700d120-8047-4f4e-8f5e-420d34909e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_search_assistant_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        assistant_model: torch.nn.Module,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        assistant_prompt_matching_window_size = 3,\n",
    "        assistant_prompt_candidate_tokens = 10,\n",
    "        assistant_draft_candidate_rounds = 4,\n",
    "        max_draft_num_candidate_tokens = 300,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "\n",
    "        assistant_model_kwargs = {}\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            \n",
    "            input_ids = input_ids.to(assistant_model.device)\n",
    "            candidate_pred_tokens, assistant_model_kwargs = assistant_model.assistant_greedy_search_pld(input_ids,\n",
    "                  stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=cur_len + max_draft_num_candidate_tokens)]),\n",
    "                  draft_num_candidate_rounds=assistant_draft_candidate_rounds,\n",
    "                  prompt_matching_window_size=assistant_prompt_matching_window_size,\n",
    "                  prompt_num_candidate_tokens = assistant_prompt_candidate_tokens,\n",
    "                  use_cache=True, \n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  eos_token_id=tokenizer.eos_token_id,\n",
    "                    print_output=False\n",
    "            )\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            candidate_pred_tokens = candidate_pred_tokens.to(self.device)\n",
    "\n",
    "            # print(candidate_pred_tokens)\n",
    "            \n",
    "            # candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if input_ids.shape[-1] > NEWLINE_THRESHOLD: # Check that there are max 5 consecutive newlines.\n",
    "                flag = True\n",
    "                for i in range(NEWLINE_THRESHOLD):\n",
    "                    if not(input_ids[0, -i] == 185): # Is a newline\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    break\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "            # New cache size - 1 because the target model generates another token not yet considered by the drafter/assistant\n",
    "            if \"past_key_values\" in assistant_model_kwargs:\n",
    "                assistant_model_kwargs[\"past_key_values\"] = _crop_past_key_values(assistant_model, assistant_model_kwargs[\"past_key_values\"], new_cache_size - 1) \n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd3b3c4-7eb8-47f2-9bec-87d0a12bf98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1202]\n",
      "[32013, 185]\n",
      "[32013, 1672]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"...\"))\n",
    "print(tokenizer.encode(\"\"\"\n",
    "\"\"\"))\n",
    "print(tokenizer.encode(\"##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "575a21f7-2aed-47ab-9f27-7c52a3c506fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the average\n",
    "average_throughput = np.mean(tokens_per_sec_arr)\n",
    "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Throughput Values')\n",
    "plt.xlabel('Tokens per Second')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "question = \"Can you please change x axis to start from 0\"\n",
    "prompt = \"[INST] Code:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:[/INST]\".format(code_text=code_text, question=question)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Move all tensor values in the inputs to GPU\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9dcce41-44b2-4260-b844-8348b785d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.greedy_search_assistant_pld = greedy_search_assistant_pld.__get__(model, type(model))\n",
    "model.greedy_search_pld = greedy_search_pld.__get__(model, type(model))\n",
    "# draft_model.greedy_search_pld = greedy_search_pld.__get__(draft_model, type(draft_model))\n",
    "draft_model.assistant_greedy_search_pld = assistant_greedy_search_pld.__get__(draft_model, type(draft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "039203dd-acd3-4e72-855b-f1f780a9d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device:  cuda:1\n",
      "Draft model device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Model device: \", model.device)\n",
    "print(\"Draft model device: \", draft_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebaeb3f8-710f-4813-81f4-df9386f21790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nuprl/CanItEdit\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da1d739d-f4fc-4b71-85b1-508a075ebb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import StoppingCriteriaList, MaxLengthCriteria\n",
    "\n",
    "# Define the variable for max_new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9881114-6ac1-4d97-9a1d-a5693321de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 105/105 [17:09<00:00,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with_assistant': [0.8294311165809631, 2.109119875356555, 2.421924028545618, 1.6313934121280909, 2.513934088870883, 4.7798324804753065, 1.2160220835357904, 1.8218904808163643, 3.348206600174308, 2.390839822590351, 7.777828121557832, 1.4169491920620203, 2.0825095623731613, 2.4952950943261385, 2.1810721587389708, 4.117371525615454, 0.764122361317277, 1.2891509514302015, 5.341210877522826, 0.9388880077749491, 2.6766153126955032, 3.927569556981325, 2.4141188841313124, 4.410751206800342, 1.110366204753518, 9.166460087522864, 1.3413185328245163, 1.4797616507858038, 1.2131869941949844, 0.6715763919055462, 2.225835405290127, 1.5070919394493103, 8.853786231949925, 1.4673749767243862, 1.289027065038681, 1.1053062193095684, 3.4879946187138557, 1.288944760337472, 0.4407064598053694, 1.9134905096143484, 2.7018794994801283, 3.921607604250312, 9.52910308726132, 2.2648291643708944, 10.96959825977683, 4.919989258050919, 3.2504062857478857, 3.0784981194883585, 0.9576174486428499, 1.5387263353914022, 6.5828643422573805, 0.9362520519644022, 0.5188384130597115, 0.5575120951980352, 2.347360797226429, 1.465178120881319, 3.059354370459914, 4.156321708112955, 2.0571422651410103, 0.8137374818325043, 0.5862619951367378, 0.2327075283974409, 4.708895977586508, 0.6554001681506634, 0.8611284755170345, 3.015057049691677, 2.592385260388255, 1.030925128608942, 0.680493451654911, 0.6031553763896227, 2.6572246327996254, 2.730660120025277, 0.7317711431533098, 0.9004631042480469, 0.8284139856696129, 0.32243893295526505, 0.7252037152647972, 0.734029795974493, 0.6913276202976704, 1.9196612797677517, 13.553791353479028, 2.447190299630165, 1.1843859646469355, 1.1172280479222536, 10.904353450983763, 5.912796596065164, 3.03754922747612, 1.0883868001401424, 1.8803406283259392, 3.1089229602366686, 1.7980430200695992, 1.4567711930721998, 2.6901126094162464, 2.093131383880973, 1.5191844534128904, 4.294516570866108, 8.15040012076497, 3.4040301255881786, 2.8012106735259295, 0.38835442811250687, 3.6220072507858276, 1.1411107312887907, 2.528551882132888, 0.6473697759211063, 1.3904501292854548], 'without_assistant': [1.0806535184383392, 5.701717780902982, 7.44548249617219, 4.7326685059815645, 5.2119478918612, 10.581640150398016, 3.226812891662121, 5.5567667577415705, 10.838368188589811, 6.382645441219211, 16.246090905740857, 4.518091792240739, 10.776563035324216, 8.850512906908989, 6.4842406921088696, 12.216658409684896, 1.3666946925222874, 2.720983935520053, 13.582706587389112, 3.6084969956427813, 8.05038221925497, 10.182425832375884, 10.23039341904223, 11.093616971746087, 2.4789181407541037, 18.0186971668154, 4.622787199914455, 3.211633352562785, 2.6129693556576967, 3.113857738673687, 5.6987315490841866, 3.997159220278263, 19.27771481871605, 5.12019076384604, 5.403752841055393, 4.757580270990729, 9.097926678135991, 4.565405556932092, 1.511710699647665, 9.411456722766161, 5.5955310594290495, 9.80021146684885, 29.14429624006152, 6.3345665000379086, 26.500861309468746, 12.48095785267651, 10.001778302714229, 8.476844768971205, 2.515516048297286, 4.251618515700102, 7.135981231927872, 2.1883048117160797, 2.0369775518774986, 1.891931252554059, 7.02446267567575, 4.20150251686573, 8.914887787774205, 12.93025148846209, 5.978118373081088, 2.8496592435985804, 3.23314923979342, 0.642380865290761, 9.276049232110381, 1.7548530958592892, 2.8472839314490557, 6.687674943357706, 7.480458192527294, 3.1911533549427986, 3.276448244228959, 2.47280759178102, 12.402144832536578, 7.319300465285778, 2.544457696378231, 3.0184653643518686, 2.9185024239122868, 1.0466033183038235, 2.543299747630954, 3.2367911972105503, 2.370978521183133, 5.987500287592411, 23.013199547305703, 6.66961207985878, 2.4527733977884054, 3.0069785956293344, 25.406910238787532, 13.899874126538634, 8.25393228419125, 2.681053441017866, 6.610466036945581, 14.143757957965136, 5.346727525815368, 3.6399740893393755, 8.78730696812272, 6.902405394241214, 4.330477561801672, 10.887805007398129, 18.466342829167843, 9.21857038512826, 11.785198003053665, 1.3124264851212502, 9.472571762278676, 4.240887198597193, 5.485037704929709, 1.9053278546780348, 4.493237411603332]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "time_taken = {\"with_assistant\": [], \"without_assistant\": []}\n",
    "outputs = {\"with_assistant\": [], \"without_assistant\": []}\n",
    "\n",
    "for row in tqdm(ds):\n",
    "    input_text = f\"# Code Before:\\n{row['before']}\\n# Instruction:\\n{row['instruction_descriptive']}\\n# Code After:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "    max_new_tokens = inputs['input_ids'].shape[-1] + 300\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    test_out = model.greedy_search_assistant_pld(inputs.input_ids,\n",
    "                    draft_model,\n",
    "                  attention_mask = inputs.attention_mask,\n",
    "                  stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                assistant_prompt_matching_window_size = 3,\n",
    "                assistant_prompt_candidate_tokens = 50,\n",
    "                assistant_draft_candidate_rounds = 4,\n",
    "                max_draft_num_candidate_tokens = 300,\n",
    "                  use_cache=True, \n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  eos_token_id=tokenizer.eos_token_id,\n",
    "                print_output=False\n",
    "            )\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    time_taken[\"with_assistant\"].append(end_time - start_time)\n",
    "    outputs[\"with_assistant\"].append(tokenizer.batch_decode(test_out))\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    test_out = model.greedy_search_pld(inputs.input_ids,\n",
    "                    draft_model,\n",
    "                  attention_mask = inputs.attention_mask,\n",
    "                  stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                prompt_matching_window_size = 3,\n",
    "                prompt_num_candidate_tokens = 50,\n",
    "                  use_cache=True, \n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  eos_token_id=tokenizer.eos_token_id,\n",
    "                 print_output=False\n",
    "            )\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    time_taken[\"without_assistant\"].append(end_time - start_time)\n",
    "    outputs[\"without_assistant\"].append(tokenizer.batch_decode(test_out))\n",
    "\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46a1571f-9dea-4ee9-8ad8-659d843c63d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278.45288597792387"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_taken['with_assistant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fef3a28-dbe6-43af-b0ae-bfe5af3c0325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750.504469525069"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_taken['without_assistant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "310f2040-4f62-4030-83a2-06a637889756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.767527336402495, 0.36990955294573236, 0.32528772041177423, 0.3447089966403081, 0.4823406029819593, 0.4517099818685026, 0.37684927027467696, 0.3278688057723756, 0.3089216514806318, 0.3745844641706517, 0.47875074482129065, 0.31361673405915624, 0.19324431690762234, 0.2819379080706422, 0.33636508302248425, 0.3370292749080518, 0.5591024575555056, 0.47378116959878713, 0.3932361229448837, 0.26018810848634366, 0.3324830100977756, 0.38572041885080915, 0.2359751756601871, 0.3975936088323508, 0.44792370772507123, 0.5087193598216697, 0.29015363996191246, 0.46075049308010185, 0.46429438277496354, 0.2156734341343406, 0.3905843583117772, 0.3770407572967268, 0.45927571370410036, 0.2865859973588494, 0.2385429354290979, 0.23232529066280933, 0.3833834611017646, 0.2823286440303959, 0.29152830624807047, 0.20331501976581862, 0.482863819498809, 0.40015540659667637, 0.3269628818198289, 0.3575349890726289, 0.41393365036997487, 0.39419965327387424, 0.32498283678871465, 0.3631655649466354, 0.3806842931059996, 0.36191542813855315, 0.9224890212440959, 0.42784352844802664, 0.2547099316737653, 0.2946788338346904, 0.334169445494365, 0.34872717914597945, 0.34317362633049453, 0.3214416758886492, 0.34411199925450303, 0.2855560655753731, 0.18132846696993074, 0.3622578768626778, 0.5076402528444961, 0.37347865168721556, 0.3024385682107875, 0.4508378584826816, 0.34655434114690375, 0.3230572191123737, 0.207692415973154, 0.24391520731105681, 0.21425524928788872, 0.37307665301846027, 0.2875941479376566, 0.2983181834327247, 0.28384899696575006, 0.308081321085266, 0.2851428408862596, 0.22677699958127545, 0.29157903124010315, 0.3206114718266932, 0.5889572775666413, 0.36691643686749187, 0.4828762272596653, 0.3715450617261303, 0.42918849039489265, 0.42538490221116676, 0.36801237554298033, 0.4059549069364819, 0.28444902641006015, 0.21980883506889068, 0.3362885075755568, 0.40021471508237894, 0.30613618247035584, 0.3032466603058849, 0.3508122214541257, 0.3944336409357108, 0.44136514718503445, 0.36925791997853447, 0.23768889354256986, 0.29590566215724323, 0.3823678871675853, 0.26907358716502744, 0.4609907931645278, 0.3397681791780134, 0.30945396423851496]\n"
     ]
    }
   ],
   "source": [
    "ratios = []\n",
    "assisted_sum = 0\n",
    "non_assisted_sum = 0\n",
    "for idx, (i, j) in enumerate(zip(time_taken['with_assistant'], time_taken['without_assistant'])):\n",
    "    ratios.append(i / j)\n",
    "    if i / j > 1:\n",
    "        print(outputs['with_assistant'][idx][0])\n",
    "        if not(outputs['with_assistant'][idx][0] == outputs['without_assistant'][idx][0]):\n",
    "            print(\"ERROR - with assistant and without assistant have different results. Without assistant:\\n\")\n",
    "            print(outputs[\"without_assistant\"][idx][0])\n",
    "        print(\"============\")\n",
    "    else:\n",
    "        assisted_sum += i\n",
    "        non_assisted_sum += j\n",
    "print(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11a4f8d5-e8b7-4eab-86fb-2c779fd5a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -62,4 +62,4 @@\n",
      "\n",
      " message = \"path/to/image.jpg\"\n",
      " message_type = \"image\"\n",
      " result = process_message(message, message_type)\n",
      "-print(result)  # Output: Processed image message with description: path/to/image.jpg<｜end▁of▁sentence｜>\n",
      "+print(result)  # Output: Processed image message with description: path/to/image.jpg<｜end▁of▁sentence｜><｜begin▁of▁sentence｜>\n",
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -62,8 +62,7 @@\n",
      "\n",
      "                     word_con_count -= 1\n",
      "                 else:\n",
      "                     total_con_count += 1\n",
      "+            if word[i] == 'x':\n",
      "+                word_con_count += 2\n",
      " \n",
      "-    return total_con_count\n",
      "-\n",
      "-# Test Cases:\n",
      "-print(consonant_within(\"The 123456789000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "+    return total_con_count<｜end▁of▁sentence｜>\n",
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -21,6 +21,7 @@\n",
      "\n",
      "     2. `cosine_similarity` should throw if one of the vectors being compared has magnitude of zero\n",
      " # Code After:\n",
      " from abc import ABC, abstractmethod\n",
      "+from math import acos, sqrt\n",
      " \n",
      " class Vector(ABC):\n",
      "     def __init__(self, *args: int):\n",
      "@@ -45,4 +46,24 @@\n",
      "\n",
      "             raise Exception(\"Vectors must have the same length\")\n",
      "         if sum(a ** 2 for a in self.vals) == 0 or sum(a ** 2 for a in other.vals) == 0:\n",
      "             raise Exception(\"Vectors must not have magnitude of zero\")\n",
      "-        return sum(a * b for a, b in zip(self.vals, other.vals)) / (sum(a ** 2 for a in self.vals) ** 0.5 * sum(a ** 2 for a in other.vals) ** 0.5)<｜end▁of▁sentence｜>\n",
      "+        return sum(a * b for a, b in zip(self.vals, other.vals)) / (sqrt(sum(a ** 2 for a in self.vals)) * sqrt(sum(a ** 2 for a in other.vals)))\n",
      "+\n",
      "+# Testing:\n",
      "+from math import pi\n",
      "+\n",
      "+v1 = MyVector(1, 2, 3)\n",
      "+v2 = MyVector(4, 5, 6)\n",
      "+v3 = MyVector(0, 0, 0)\n",
      "+\n",
      "+assert v1.manhattan_distance(v2) == 9\n",
      "+assert v1.cosine_similarity(v2) == acos(2 / 3)\n",
      "+\n",
      "+try:\n",
      "+    v1.cosine_similarity(v3)\n",
      "+except Exception as e:\n",
      "+    assert str(e) == \"Vectors must not have magnitude of zero\"\n",
      "+\n",
      "+try:\n",
      "+    v1.manhattan_distance(v3)\n",
      "+except Exception as e:\n",
      "+    assert str(e) == \"Vectors must have the same length\"<｜end▁of▁sentence｜>\n",
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -66,4 +66,4 @@\n",
      "\n",
      "         return [c[0] for c in clusters]\n",
      " <jupyter_output>\n",
      " <empty_output>\n",
      "-<｜end▁of▁sentence｜>\n",
      "+<｜end▁of▁sentence｜><｜begin▁of▁sentence｜>\n",
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -102,13 +102,14 @@\n",
      "\n",
      "             return (2, 2)\n",
      "         else:\n",
      "             raise Exception\n",
      "-\n",
      "+        \n",
      " class GoodStrategy(Strategy):\n",
      "     def returnMove(self, board: List[List[bool]]) -> Tuple[int, int]:\n",
      "         for i in range(3):\n",
      "             for j in range(3):\n",
      "                 if board[i][j] == None:\n",
      "                     return (i, j)\n",
      "+        raise Exception\n",
      " \n",
      " class Game:\n",
      "     def __init__(self, player1: Strategy, player2: Strategy):\n",
      "Discrepancy: \n",
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -66,4 +66,4 @@\n",
      "\n",
      "     return pd.concat([data, properties_columns], axis=1)\n",
      " <jupyter_output>\n",
      " <empty_output>\n",
      "-<｜end▁of▁sentence｜>\n",
      "+<｜end▁of▁sentence｜><｜begin▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "for wa, woa in zip(outputs['with_assistant'], outputs['without_assistant']):\n",
    "    if not(wa[0] == woa[0]):\n",
    "        print(\"Discrepancy: \")\n",
    "        print(\"\\n\".join(difflib.unified_diff(woa[0].splitlines(), wa[0].splitlines())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38371787-d786-44db-b054-b5a4861058b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278.45288597792387 750.504469525069 0.37102095628308945\n"
     ]
    }
   ],
   "source": [
    "print(assisted_sum, non_assisted_sum, assisted_sum/non_assisted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5567176-474d-46f5-9491-66505503ad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYn0lEQVR4nO3de5CVBf348Q8X2SVnuXiBQFEQzRsqJOIoamqUk2g6zZSOaESllWumzJiQKaEJ2DBG4wWVEJlGxUosRwwtihzGHBWk8YoBmqShabZ4GVdgn98fv3HnS4L2nP2cs7vwes08f+zDOXs+H84C73nOLqdLURRFAAAk6NreAwAA2w9hAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCk6V7rB2xpaYlXXnklGhoaokuXLrV+eACgAkVRxFtvvRUDBw6Mrl23fV2i5mHxyiuvxKBBg2r9sABAgnXr1sWee+65zV+veVg0NDRExP8frFevXrV+eACgAhs2bIhBgwa1/ju+LTUPiw9e/ujVq5ewAIBO5uO+jcE3bwIAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJCm5m+bTr7BkxZt9fyLM8bWeBIAdnSuWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJCmVFhs3rw5Lr/88hgyZEj07Nkzhg4dGldddVUURVGt+QCATqR7mRtfc801MXv27Jg/f34cfPDB8fjjj8eECROid+/eceGFF1ZrRgCgkygVFg8//HCcdtppMXbs2IiIGDx4cNx5553x6KOPVmU4AKBzKfVSyNFHHx1LliyJ559/PiIi/vrXv8ayZcviC1/4wjbv09zcHBs2bNjiAAC2T6WuWEyaNCk2bNgQBxxwQHTr1i02b94cV199dYwbN26b95k+fXpMnTq1zYOyYxg8adFWz784Y2yNJwGgEqWuWPzyl7+M22+/Pe64445YsWJFzJ8/P2bOnBnz58/f5n0mT54cTU1Nrce6devaPDQA0DGVumJxySWXxKRJk+LMM8+MiIhDDjkk/v73v8f06dNj/PjxW71PXV1d1NXVtX1SAKDDK3XF4t13342uXbe8S7du3aKlpSV1KACgcyp1xeLUU0+Nq6++Ovbaa684+OCD44knnohrr702vv71r1drPgCgEykVFtddd11cfvnlcf7558drr70WAwcOjG9961txxRVXVGs+AKATKRUWDQ0NMWvWrJg1a1aVxgEAOjPvFQIApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApOne3gPQsQ2etOhD516cMbYdJgGgM3DFAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIUzosXn755Tj77LNj1113jZ49e8YhhxwSjz/+eDVmAwA6me5lbvzmm2/G6NGj44QTTojf/e53sfvuu8ff/va36Nu3b7XmAwA6kVJhcc0118SgQYNi3rx5reeGDBmSPhQA0DmVeink3nvvjZEjR8aXv/zl6NevX4wYMSLmzJlTrdkAgE6mVFisXbs2Zs+eHfvtt1888MAD8Z3vfCcuvPDCmD9//jbv09zcHBs2bNjiAAC2T6VeCmlpaYmRI0fGtGnTIiJixIgR8dRTT8VNN90U48eP3+p9pk+fHlOnTm37pKQYPGnRVs+/OGNsjScBYHtU6orFgAED4qCDDtri3IEHHhgvvfTSNu8zefLkaGpqaj3WrVtX2aQAQIdX6orF6NGjY9WqVVuce/7552Pvvffe5n3q6uqirq6usukAgE6l1BWLiy++OB555JGYNm1arF69Ou6444645ZZborGxsVrzAQCdSKmwOOKII+Kee+6JO++8M4YNGxZXXXVVzJo1K8aNG1et+QCATqTUSyEREaecckqccsop1ZgFAOjkvFcIAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJBGWAAAaYQFAJCme3sPQOczeNKiUrd/ccbYqjxmxucFIJcrFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKQRFgBAGmEBAKRpU1jMmDEjunTpEhdddFHSOABAZ1ZxWDz22GNx8803x6GHHpo5DwDQiVUUFm+//XaMGzcu5syZE3379s2eCQDopCoKi8bGxhg7dmyMGTPmY2/b3NwcGzZs2OIAALZP3cveYcGCBbFixYp47LHH/qfbT58+PaZOnVp6MLZvgyctqunneHHG2DY/XoZtzbyt+bZ2+46yC8DWlLpisW7duvje974Xt99+e9TX1/9P95k8eXI0NTW1HuvWratoUACg4yt1xWL58uXx2muvxac//enWc5s3b46HHnoorr/++mhubo5u3bptcZ+6urqoq6vLmRYA6NBKhcVnP/vZePLJJ7c4N2HChDjggAPi0ksv/VBUAAA7llJh0dDQEMOGDdvi3M477xy77rrrh84DADse//MmAJCm9E+F/LelS5cmjAEAbA9csQAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACCNsAAA0ggLACBN9/YeYHs0eNKirZ5/ccbYDjEHH29rv3e1fv4AOiNXLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEgjLACANMICAEhTKiymT58eRxxxRDQ0NES/fv3i9NNPj1WrVlVrNgCgkykVFn/+85+jsbExHnnkkfj9738fGzdujM9//vPxzjvvVGs+AKAT6V7mxosXL97i49tuuy369esXy5cvj+OOOy51MACg8ykVFv+tqakpIiJ22WWXbd6mubk5mpubWz/esGFDWx4SAOjAKg6LlpaWuOiii2L06NExbNiwbd5u+vTpMXXq1EofpmoGT1rU3iNUpDPO3RlnLqMj77et2V6cMbbGkwA7iop/KqSxsTGeeuqpWLBgwUfebvLkydHU1NR6rFu3rtKHBAA6uIquWFxwwQVx3333xUMPPRR77rnnR962rq4u6urqKhoOAOhcSoVFURTx3e9+N+65555YunRpDBkypFpzAQCdUKmwaGxsjDvuuCN++9vfRkNDQ6xfvz4iInr37h09e/asyoAAQOdR6nssZs+eHU1NTXH88cfHgAEDWo+77rqrWvMBAJ1I6ZdCAAC2xXuFAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkKZ7ew+QafCkRe09wkfqyPN15NkydPT9Ovp8ZWxtlxdnjK3556D9bOvr2XOYq6P+PrtiAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQJqKwuKGG26IwYMHR319fRx55JHx6KOPZs8FAHRCpcPirrvuiokTJ8aUKVNixYoVcdhhh8VJJ50Ur732WjXmAwA6kdJhce2118a5554bEyZMiIMOOihuuumm+MQnPhG33nprNeYDADqR7mVu/P7778fy5ctj8uTJree6du0aY8aMib/85S9bvU9zc3M0Nze3ftzU1BQRERs2bKhk3o/U0vxu+ueED2zra7bWX3dl/uxsa7Zq/Pn7qMcs+3gZn4P20x5fdzuiWv8+f/B5i6L46BsWJbz88stFRBQPP/zwFucvueSSYtSoUVu9z5QpU4qIcDgcDofDsR0c69at+8hWKHXFohKTJ0+OiRMntn7c0tIS//73v2PXXXeNLl26VPvht2nDhg0xaNCgWLduXfTq1avd5qiVHW3fCDvbeftl5+1/5464b1EU8dZbb8XAgQM/8nalwmK33XaLbt26xauvvrrF+VdffTU++clPbvU+dXV1UVdXt8W5Pn36lHnYqurVq1eHedJqYUfbN8LOOwo77xh2tJ072r69e/f+2NuU+ubNHj16xOGHHx5LlixpPdfS0hJLliyJo446qvyEAMB2pfRLIRMnTozx48fHyJEjY9SoUTFr1qx45513YsKECdWYDwDoREqHxRlnnBH/+te/4oorroj169fH8OHDY/HixdG/f/9qzFc1dXV1MWXKlA+9TLO92tH2jbDzjsLOO4YdbefOvG+X4mN/bgQA4H/jvUIAgDTCAgBIIywAgDTCAgBIs92GRZm3dp8zZ04ce+yx0bdv3+jbt2+MGTOmU74VfJmdFy5cGCNHjow+ffrEzjvvHMOHD49f/OIXNZw2R5md/68FCxZEly5d4vTTT6/ugFVQZufbbrstunTpssVRX19fw2lzlH2e//Of/0RjY2MMGDAg6urq4lOf+lTcf//9NZo2R5mdjz/++A89z126dImxY8fWcOK2Kfscz5o1K/bff//o2bNnDBo0KC6++OJ47733ajRtjjI7b9y4Ma688soYOnRo1NfXx2GHHRaLFy+u4bQllHmvkM5iwYIFRY8ePYpbb721ePrpp4tzzz236NOnT/Hqq69u9fZnnXVWccMNNxRPPPFE8eyzzxZf+9rXit69exf/+Mc/ajx55cru/Kc//alYuHBh8cwzzxSrV68uZs2aVXTr1q1YvHhxjSevXNmdP/DCCy8Ue+yxR3HssccWp512Wm2GTVJ253nz5hW9evUq/vnPf7Ye69evr/HUbVN25+bm5mLkyJHFySefXCxbtqx44YUXiqVLlxYrV66s8eSVK7vzG2+8scVz/NRTTxXdunUr5s2bV9vBK1R239tvv72oq6srbr/99uKFF14oHnjggWLAgAHFxRdfXOPJK1d25+9///vFwIEDi0WLFhVr1qwpbrzxxqK+vr5YsWJFjSf/eNtlWIwaNapobGxs/Xjz5s3FwIEDi+nTp/9P99+0aVPR0NBQzJ8/v1ojpmvrzkVRFCNGjCh++MMfVmO8qqhk502bNhVHH3108fOf/7wYP358pwuLsjvPmzev6N27d42mq46yO8+ePbvYZ599ivfff79WI6Zr65/nn/70p0VDQ0Px9ttvV2vEVGX3bWxsLE488cQtzk2cOLEYPXp0VefMVHbnAQMGFNdff/0W5770pS8V48aNq+qcldjuXgr54K3dx4wZ03ru497a/b+9++67sXHjxthll12qNWaqtu5cFEUsWbIkVq1aFccdd1w1R01T6c5XXnll9OvXL77xjW/UYsxUle789ttvx9577x2DBg2K0047LZ5++ulajJuikp3vvffeOOqoo6KxsTH69+8fw4YNi2nTpsXmzZtrNXabZPwdNnfu3DjzzDNj5513rtaYaSrZ9+ijj47ly5e3vnSwdu3auP/+++Pkk0+uycxtVcnOzc3NH3oZs2fPnrFs2bKqzlqJqr+7aa29/vrrsXnz5g/9T6D9+/eP55577n/6HJdeemkMHDhwiye9I6t056ampthjjz2iubk5unXrFjfeeGN87nOfq/a4KSrZedmyZTF37txYuXJlDSbMV8nO+++/f9x6661x6KGHRlNTU8ycOTOOPvroePrpp2PPPfesxdhtUsnOa9eujT/+8Y8xbty4uP/++2P16tVx/vnnx8aNG2PKlCm1GLtN2vp32KOPPhpPPfVUzJ07t1ojpqpk37POOitef/31OOaYY6Ioiti0aVN8+9vfjh/84Ae1GLnNKtn5pJNOimuvvTaOO+64GDp0aCxZsiQWLlzYIYN5u7ti0VYzZsyIBQsWxD333NMpv8mtjIaGhli5cmU89thjcfXVV8fEiRNj6dKl7T1WVbz11ltxzjnnxJw5c2K33XZr73Fq5qijjoqvfvWrMXz48PjMZz4TCxcujN133z1uvvnm9h6talpaWqJfv35xyy23xOGHHx5nnHFGXHbZZXHTTTe192g1MXfu3DjkkENi1KhR7T1K1SxdujSmTZsWN954Y6xYsSIWLlwYixYtiquuuqq9R6uan/3sZ7HffvvFAQccED169IgLLrggJkyYEF27drx/xre7KxaVvLX7B2bOnBkzZsyIP/zhD3HooYdWc8xUle7ctWvX2HfffSMiYvjw4fHss8/G9OnT4/jjj6/muCnK7rxmzZp48cUX49RTT20919LSEhER3bt3j1WrVsXQoUOrO3QbteVr+wM77bRTjBgxIlavXl2NEdNVsvOAAQNip512im7durWeO/DAA2P9+vXx/vvvR48ePao6c1u15Xl+5513YsGCBXHllVdWc8RUlex7+eWXxznnnBPf/OY3IyLikEMOiXfeeSfOO++8uOyyyzrkP7b/VyU777777vGb3/wm3nvvvXjjjTdi4MCBMWnSpNhnn31qMXIpHft3vwKVvrX7T37yk7jqqqti8eLFMXLkyFqMmibr7exbWlqiubm5GiOmK7vzAQccEE8++WSsXLmy9fjiF78YJ5xwQqxcuTIGDRpUy/ErkvE8b968OZ588skYMGBAtcZMVcnOo0ePjtWrV7eGY0TE888/HwMGDOjwURHRtuf5V7/6VTQ3N8fZZ59d7THTVLLvu++++6F4+CAki07w9ldteY7r6+tjjz32iE2bNsXdd98dp512WrXHLa+dv3m0KhYsWFDU1dUVt912W/HMM88U5513XtGnT5/WH7M755xzikmTJrXefsaMGUWPHj2KX//611v8yNZbb73VXiuUVnbnadOmFQ8++GCxZs2a4plnnilmzpxZdO/evZgzZ057rVBa2Z3/W2f8qZCyO0+dOrV44IEHijVr1hTLly8vzjzzzKK+vr54+umn22uF0sru/NJLLxUNDQ3FBRdcUKxataq47777in79+hU//vGP22uF0ir92j7mmGOKM844o9bjtlnZfadMmVI0NDQUd955Z7F27driwQcfLIYOHVp85Stfaa8VSiu78yOPPFLcfffdxZo1a4qHHnqoOPHEE4shQ4YUb775ZjttsG3bZVgURVFcd911xV577VX06NGjGDVqVPHII4+0/tpnPvOZYvz48a0f77333kVEfOiYMmVK7QdvgzI7X3bZZcW+++5b1NfXF3379i2OOuqoYsGCBe0wdduU2fm/dcawKIpyO1900UWtt+3fv39x8sknd8ife/84ZZ/nhx9+uDjyyCOLurq6Yp999imuvvrqYtOmTTWeum3K7vzcc88VEVE8+OCDNZ40R5l9N27cWPzoRz8qhg4dWtTX1xeDBg0qzj///A75j+xHKbPz0qVLiwMPPLCoq6srdt111+Kcc84pXn755XaY+uN523QAIM129z0WAED7ERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQBphAQCkERYAQJr/B1NosFtl1ZgRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ratios, bins=80)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
