{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e94e4e-8ca0-462a-9f41-95a2f41a944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers # requires transformers==4.35.2\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1668d6c1-d608-4f91-8741-b33693e83d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e27a50-63e6-45c1-88c2-e43d2fa94bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f1eb0355d3462c8981c88754cb5b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16) # , use_flash_attention=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640f2d62-7976-4d26-beb1-08aa01043736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n",
    "from transformers.models.auto import (\n",
    "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
    "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
    ")\n",
    "from transformers.utils import ExplicitEnum, ModelOutput, is_accelerate_available, logging\n",
    "from transformers.generation.beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from transformers.generation.logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    EncoderRepetitionPenaltyLogitsProcessor,\n",
    "    EpsilonLogitsWarper,\n",
    "    EtaLogitsWarper,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    MinNewTokensLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SequenceBiasLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    "    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "\n",
    "from transformers.generation.utils import _crop_past_key_values\n",
    "import difflib\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using greedy search.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
    "            if all batches finished early due to the `eos_token_id`.\n",
    "        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
    "            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
    "        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5466bcc-865b-4333-9cae-4d3fa4afd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:end_idx]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
    "UNDERLINE = \"\\x1b[4m\"\n",
    "RESET = \"\\x1b[0m\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_search_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        draft_matching_window_size = 3,\n",
    "        draft_num_candidate_tokens = 10,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "            \n",
    "            if input_ids[0, -1] == 185 and input_ids[0, -2] == 185 and input_ids[0, -3] == 185: # hacky stopping criteria for new-line ending models\n",
    "                break\n",
    "            \n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd3b3c4-7eb8-47f2-9bec-87d0a12bf98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1202]\n",
      "[32013, 185]\n",
      "[32013, 1672]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"...\"))\n",
    "print(tokenizer.encode(\"\"\"\n",
    "\"\"\"))\n",
    "print(tokenizer.encode(\"##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "575a21f7-2aed-47ab-9f27-7c52a3c506fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the average\n",
    "average_throughput = np.mean(tokens_per_sec_arr)\n",
    "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Throughput Values')\n",
    "plt.xlabel('Tokens per Second')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "end_text = '\\nprint(\"END\")'\n",
    "code_text += end_text\n",
    "\n",
    "question = \"Can you please change x axis to start from 0\"\n",
    "prompt = \"[INST] Code:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:[/INST]\".format(code_text=code_text, question=question)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "code_inputs = tokenizer(code_text, return_tensors=\"pt\")\n",
    "end_tokens = tokenizer(end_text, return_tensors=\"pt\")\n",
    "\n",
    "# Move all tensor values in the inputs to GPU\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(device)\n",
    "    code_inputs[key] = code_inputs[key].to(device)\n",
    "    end_tokens[key] = end_tokens[key].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7068b630-c984-4665-b69a-921a83f7ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_search_pld_custom(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        end_sequence: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        draft_matching_window_size = 3,\n",
    "        draft_num_candidate_tokens = 10,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "        original_input_length = input_ids.shape[-1]\n",
    "        all_tokens = torch.Tensor([[]])\n",
    "        all_tokens.to(input_ids.device)\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "\n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            first_newline = -1\n",
    "            last_newline = 0\n",
    "            for i in range(n_matches + 1):\n",
    "                if valid_tokens[0, i] == 185:\n",
    "                    if first_newline < 0:\n",
    "                        first_newline = i\n",
    "                    last_newline = i\n",
    "\n",
    "            new_cache_size = -1\n",
    "            all_tokens = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            if valid_tokens.shape[1] > 30: # just arbitrarily crop and use ellipses\n",
    "                # print(first_newline, last_newline)\n",
    "                st = valid_tokens.shape[1]\n",
    "                # print(tokenizer.batch_decode(valid_tokens[:, :first_newline]))\n",
    "                new_cache_size = first_newline + input_ids.shape[1]\n",
    "                valid_tokens = torch.cat((valid_tokens[:, :first_newline], \n",
    "                                          torch.as_tensor([[185, 1202, 185]], dtype=torch.int32, device=input_ids.device), \n",
    "                                          valid_tokens[:, last_newline:]\n",
    "                                         ), dim=-1) \n",
    "                # print(\"Token delta: \", st, valid_tokens.shape[1])\n",
    "                \n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if input_ids[0, -1] == 185 and input_ids[0, -2] == 185 and input_ids[0, -3] == 185: # hacky stopping criteria for new-line ending models\n",
    "                break\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1 if new_cache_size == -1 else new_cache_size\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            end_seq_len = end_sequence.shape[1]\n",
    "            code_end_flag = False\n",
    "            for i in range(all_tokens.shape[1] - end_seq_len + 1):\n",
    "                if torch.equal(all_tokens[0, i:i+end_seq_len], end_sequence):\n",
    "                    code_end_flag = True\n",
    "                    break\n",
    "\n",
    "            if code_end_flag:\n",
    "                break\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9dcce41-44b2-4260-b844-8348b785d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.greedy_search_pld = greedy_search_pld.__get__(model, type(model))\n",
    "# model.greedy_search_pld_diff = greedy_search_pld_diff.__get__(model, type(model))\n",
    "model.greedy_search_pld_custom = greedy_search_pld_custom.__get__(model, type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9527e26-56e7-48ff-bb11-724f88ea0fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```\u001b[31mpython\n",
      "...\n",
      "\n",
      "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edge\u001b[0m\u001b[32mcolor='black', alpha=0.7)\n",
      "...\n",
      "\n",
      "plt.text(average_throughput*0.9, max(plt.ylim\u001b[0m\u001b[34m())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      "...\n",
      "\n",
      "plt.x\u001b[0mlim(0\u001b[35m, max\u001b[0m\u001b[31m(tok\u001b[0m\u001b[32mens_per_sec_arr))\u001b[0m\n",
      "pl\u001b[34mt.show\u001b[0m\u001b[35m()\n",
      "\n",
      "print(\"END\")\n",
      "\u001b[0m```\n",
      "\n",
      "\n",
      "\n",
      "Total time: 0.9307379722595215 seconds\n",
      "Tokens per second: 147.19502597214301 tokens/sec\n",
      "Total tokens generated: 137\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import StoppingCriteriaList, MaxLengthCriteria\n",
    "\n",
    "# Define the variable for max_new_tokens\n",
    "max_new_tokens = 1000\n",
    "generate_type = 'custom'\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate the output\n",
    "\n",
    "\n",
    "if generate_type == 'regular':\n",
    "    out = model.generate(inputs=inputs.input_ids, max_new_tokens=max_new_tokens, use_cache=True, pad_token_id=0,\n",
    "                         do_sample=False,\n",
    "                         return_dict_in_generate=True)\n",
    "elif generate_type == 'pld':\n",
    "    out = model.greedy_search_pld(inputs.input_ids, \n",
    "                          attention_mask = inputs.attention_mask,\n",
    "                          stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                          draft_matching_window_size = 3,\n",
    "                          draft_num_candidate_tokens = 50,\n",
    "                          use_cache=True, \n",
    "                          pad_token_id=tokenizer.pad_token_id,\n",
    "                          eos_token_id=tokenizer.eos_token_id,\n",
    "                          return_dict_in_generate=True)\n",
    "elif generate_type == 'custom':\n",
    "    out = model.greedy_search_pld_custom(inputs.input_ids, \n",
    "                                end_tokens.input_ids,\n",
    "                              attention_mask = inputs.attention_mask,\n",
    "                              stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                              draft_matching_window_size = 3,\n",
    "                              draft_num_candidate_tokens = 100,\n",
    "                              use_cache=True, \n",
    "                              pad_token_id=tokenizer.pad_token_id,\n",
    "                              eos_token_id=tokenizer.eos_token_id,\n",
    "                              return_dict_in_generate=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "out_text = tokenizer.batch_decode(out.sequences, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "num_tokens_generated = len(out.sequences[0]) - len(inputs['input_ids'][0])\n",
    "\n",
    "total_time = end_time - start_time\n",
    "tokens_per_sec = num_tokens_generated / total_time\n",
    "\n",
    "print(f\"\\n\\nTotal time: {total_time} seconds\")\n",
    "print(f\"Tokens per second: {tokens_per_sec} tokens/sec\")\n",
    "print(f\"Total tokens generated: {num_tokens_generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de14076a-0985-42a7-b104-d6f92b5a5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nuprl/CanItEdit\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0997c4ad-c43f-4933-a647-82768495808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'name', 'full_name', 'before', 'after', 'tests', 'instruction_descriptive', 'instruction_lazy', 'taxonomy'],\n",
      "    num_rows: 105\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3527769-5cec-4bac-ad26-4c81f8464738",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_counts = [len(tokens) for tokens in tokenizer(ds['before'])['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61c7f48d-8ab7-4fca-b0a9-f9387845d7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.hist(input_token_counts, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b403d30d-ffb6-4236-938c-82fc0379ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.add_column(\"token_count\", input_token_counts)\n",
    "ds = ds.filter(lambda example: example['token_count'] > 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f297529f-7a97-46d4-bd51-491c09dfc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 4000\n",
    "total_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5fc71c5-817f-4405-ba75-7a9dfa7c2b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                      | 0/31 [00:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     code_inputs[key] \u001b[38;5;241m=\u001b[39m code_inputs[key]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 13\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search_pld_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mend_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStoppingCriteriaList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMaxLengthCriteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdraft_matching_window_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdraft_num_candidate_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                          \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                          \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mprint_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     25\u001b[0m total_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 68\u001b[0m, in \u001b[0;36mgreedy_search_pld_custom\u001b[0;34m(self, input_ids, end_sequence, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, draft_matching_window_size, draft_num_candidate_tokens, print_output, **model_kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(candidate_input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcandidate_kwargs)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m new_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39mcandidate_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m :]  \u001b[38;5;66;03m# excludes the input prompt if present\u001b[39;00m\n\u001b[1;32m     77\u001b[0m selected_tokens \u001b[38;5;241m=\u001b[39m new_logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1034\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:922\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    912\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    913\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         use_cache,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:672\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    683\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:406\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    407\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:1887\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1885\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1887\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for row in tqdm(ds):\n",
    "    inputs = tokenizer(f\"## Code Before:\\n{row['before']}{end_text}\\n## Instruction:\\n{row['instruction_descriptive']}\\n## Code After:\\n\", return_tensors=\"pt\")\n",
    "    code_inputs = tokenizer(row['before'], return_tensors=\"pt\")\n",
    "\n",
    "    # Move all tensor values in the inputs to GPU\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "        code_inputs[key] = code_inputs[key].to(device)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    out = model.greedy_search_pld_custom(inputs.input_ids, \n",
    "                            end_tokens.input_ids,\n",
    "                              attention_mask = inputs.attention_mask,\n",
    "                              stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                              draft_matching_window_size = 3,\n",
    "                              draft_num_candidate_tokens = 70,\n",
    "                              use_cache=True, \n",
    "                              pad_token_id=tokenizer.pad_token_id,\n",
    "                              eos_token_id=tokenizer.eos_token_id,\n",
    "                              return_dict_in_generate=True,\n",
    "                                print_output=False)\n",
    "    end_time = time.perf_counter()\n",
    "    total_time += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696f669-ae63-4a95-833d-e65f300a306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dd010-e805-49d6-a4a6-566ae7fed811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
