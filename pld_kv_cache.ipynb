{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a78a8f-4f43-4993-8253-af11ce43625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers # requires transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9644fa5-2ddb-4b83-b4cc-aa47a34ca714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6821c46f9f643349025775ea402bc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.float16, use_flash_attention_2=True) # , use_flash_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240271f5-9461-4f3a-8cf6-47143a3a73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n",
    "from transformers.models.auto import (\n",
    "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
    "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
    ")\n",
    "from transformers.utils import ExplicitEnum, ModelOutput, is_accelerate_available, logging\n",
    "from transformers.generation.beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from transformers.generation.logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    EncoderRepetitionPenaltyLogitsProcessor,\n",
    "    EpsilonLogitsWarper,\n",
    "    EtaLogitsWarper,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    MinNewTokensLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SequenceBiasLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    "    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "\n",
    "# from transformers.generation.utils import _crop_past_key_values\n",
    "import difflib\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using greedy search.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
    "            if all batches finished early due to the `eos_token_id`.\n",
    "        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
    "            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
    "        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e47abd-bc0a-4bac-b28d-1bdca9722919",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            # if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "            #     return input_ids[0, start_idx:end_idx]\n",
    "            if start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:min(end_idx, input_length)]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec54fab-3936-4070-b602-08e7cfc345c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_cache_crop_with_start(self, start: int, max_length: int):\n",
    "    \"\"\"Crop the past key values up to a new `max_length` in terms of tokens. `max_length` can also be\n",
    "    negative to remove `max_length` tokens. This is used in assisted decoding and contrastive search.\"\"\"\n",
    "\n",
    "    # In case it is negative\n",
    "    if max_length < 0:\n",
    "        max_length = self.get_seq_length() - abs(max_length)\n",
    "\n",
    "    if self.get_seq_length() <= max_length:\n",
    "        return\n",
    "\n",
    "    self._seen_tokens = max_length\n",
    "    for idx in range(len(self.key_cache)):\n",
    "        self.key_cache[idx] = self.key_cache[idx][..., start:max_length, :]\n",
    "        self.value_cache[idx] = self.value_cache[idx][..., start:max_length, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181b0463-b789-4bce-9e83-77970a881caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cache_utils\n",
    "from cache_utils import DynamicCache\n",
    "\n",
    "def _crop_past_key_values(model, past_key_values, max_length):\n",
    "    \"\"\"Crops the past key values up to a certain maximum length.\"\"\"\n",
    "    new_past = []\n",
    "    if model.config.is_encoder_decoder:\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, :max_length, :],\n",
    "                    past_key_values[idx][1][:, :, :max_length, :],\n",
    "                    past_key_values[idx][2],\n",
    "                    past_key_values[idx][3],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    # bloom is special\n",
    "    elif \"bloom\" in model.__class__.__name__.lower() or (\n",
    "        model.config.architectures is not None and \"bloom\" in model.config.architectures[0].lower()\n",
    "    ):\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, :max_length],\n",
    "                    past_key_values[idx][1][:, :max_length, :],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    # gptbigcode is too\n",
    "    elif \"gptbigcode\" in model.__class__.__name__.lower() or (\n",
    "        model.config.architectures is not None and \"gptbigcode\" in model.config.architectures[0].lower()\n",
    "    ):\n",
    "        if model.config.multi_query:\n",
    "            for idx in range(len(past_key_values)):\n",
    "                past_key_values[idx] = past_key_values[idx][:, :max_length, :]\n",
    "        else:\n",
    "            for idx in range(len(past_key_values)):\n",
    "                past_key_values[idx] = past_key_values[idx][:, :, :max_length, :]\n",
    "    elif isinstance(past_key_values, DynamicCache):\n",
    "        past_key_values.crop(max_length)\n",
    "\n",
    "    elif past_key_values is not None:\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, :max_length, :],\n",
    "                    past_key_values[idx][1][:, :, :max_length, :],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    return past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "681e8337-14cd-49c0-ac29-c6cd6182e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop_past_key_values_with_start(model, past_key_values, start, max_length):\n",
    "    \"\"\"Crops the past key values up to a certain maximum length.\"\"\"\n",
    "    new_past = []\n",
    "    if model.config.is_encoder_decoder:\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, start:max_length, :],\n",
    "                    past_key_values[idx][1][:, :, start:max_length, :],\n",
    "                    past_key_values[idx][2],\n",
    "                    past_key_values[idx][3],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    # bloom is special\n",
    "    elif \"bloom\" in model.__class__.__name__.lower() or (\n",
    "        model.config.architectures is not None and \"bloom\" in model.config.architectures[0].lower()\n",
    "    ):\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, start:max_length],\n",
    "                    past_key_values[idx][1][:, start:max_length, :],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    # gptbigcode is too\n",
    "    elif \"gptbigcode\" in model.__class__.__name__.lower() or (\n",
    "        model.config.architectures is not None and \"gptbigcode\" in model.config.architectures[0].lower()\n",
    "    ):\n",
    "        if model.config.multi_query:\n",
    "            for idx in range(len(past_key_values)):\n",
    "                past_key_values[idx] = past_key_values[idx][:, start:max_length, :]\n",
    "        else:\n",
    "            for idx in range(len(past_key_values)):\n",
    "                past_key_values[idx] = past_key_values[idx][:, :, start:max_length, :]\n",
    "    elif isinstance(past_key_values, DynamicCache):\n",
    "        past_key_values.crop(max_length)\n",
    "\n",
    "    elif past_key_values is not None:\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, start:max_length, :],\n",
    "                    past_key_values[idx][1][:, :, start:max_length, :],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    return past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76f1b66b-f389-4d36-9501-4954038ecc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
    "UNDERLINE = \"\\x1b[4m\"\n",
    "RESET = \"\\x1b[0m\"\n",
    "NEWLINE_THRESHOLD = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86aaba5b-ca5d-42ec-8a23-3e3b9b678c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 185]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"\"\"\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b350731-eb25-4357-80d2-37024cf29c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import numpy as np\n",
    "@torch.no_grad()\n",
    "def greedy_search_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        draft_matching_window_size = 3,\n",
    "        draft_num_candidate_tokens = 10,\n",
    "        print_output=True,\n",
    "        kv_tolerance=15,\n",
    "        use_kv_compression=False,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "        start_length = input_ids.shape[-1]\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "            \n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "\n",
    "            original_code_index = 0\n",
    "            if (\"past_key_values\" in model_kwargs) and use_kv_compression and (i % 3 == 0):\n",
    "                # pre-generation, you should be cropping the inputs\n",
    "                # print(input_ids.shape, start_length, input_ids[0, :start_length].tolist(), input_ids[0, start_length:].tolist())\n",
    "                sequence_matcher = difflib.SequenceMatcher(None, input_ids[0, :start_length].tolist(), input_ids[0, start_length:].tolist())\n",
    "                # print(list(sequence_matcher.get_opcodes()))\n",
    "                deleted = inserted = same = replaced = 0\n",
    "                last_deleted = 0\n",
    "                for tag, i1, i2, j1, h2 in sequence_matcher.get_opcodes():\n",
    "                    if tag == 'delete':\n",
    "                        deleted += i2 - i1\n",
    "                        last_deleted = i2 - i1\n",
    "                    elif tag == 'insert':\n",
    "                        inserted += j2 - j1\n",
    "                    elif tag == 'equal':\n",
    "                        same += i2 - i1\n",
    "                    elif tag == 'replace':\n",
    "                        replaced += i2 - i1\n",
    "                # original_code_index = deleted + inserted + replaced - last_deleted\n",
    "                # print(\"original code index no tolerance: \", original_code_index, deleted, inserted, replaced, last_deleted)\n",
    "                original_code_index = max(0, input_ids.shape[1] - start_length - kv_tolerance)\n",
    "                # original_code_index = 0\n",
    "                \n",
    "                cropped_input_ids = input_ids[:, original_code_index:]\n",
    "                \n",
    "                candidate_input_ids = torch.cat((cropped_input_ids, candidate_pred_tokens), dim=1)\n",
    "                \n",
    "                candidate_length = candidate_input_ids.shape[1] - cropped_input_ids.shape[1]\n",
    "                \n",
    "                candidate_kwargs = copy.copy(model_kwargs)\n",
    "                # candidate_kwargs[\"past_key_values\"] = _crop_past_key_values(self, outputs.past_key_values, input_ids.shape[1])\n",
    "                candidate_kwargs[\"past_key_values\"] = _crop_past_key_values_with_start(self, model_kwargs[\"past_key_values\"], original_code_index, input_ids.shape[1])\n",
    "                # print(candidate_input_ids.shape, input_ids.shape[1] - original_code_index, original_code_index)\n",
    "            else:\n",
    "                candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "                candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "                candidate_kwargs = copy.copy(model_kwargs)\n",
    "            \n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if input_ids.shape[-1] > NEWLINE_THRESHOLD: # Check that there are max 5 consecutive newlines.\n",
    "                flag = True\n",
    "                for i in range(NEWLINE_THRESHOLD):\n",
    "                    if not(input_ids[0, -i] == 185): # Is a newline\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    print(\"Flag true\")\n",
    "                    break\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            \n",
    "            new_cache_size = new_cur_len - 1    \n",
    "            # if \"past_key_values\" in model_kwargs:\n",
    "            #     print(len(model_kwargs[\"past_key_values\"][0][0][0][0]), original_code_index + new_cache_size)\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, original_code_index + new_cache_size)\n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "da94dea5-6b40-4615-881f-4ee053e87aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.greedy_search_pld = greedy_search_pld.__get__(model, type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "215ed2e3-6bca-47b7-9055-375dfbeb1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the average\n",
    "average_throughput = np.mean(tokens_per_sec_arr)\n",
    "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Throughput Values')\n",
    "plt.xlabel('Tokens per Second')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "question = \"Can you please change x axis to start from 0\"\n",
    "prompt = \"[INST] Code:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:[/INST]\".format(code_text=code_text, question=question)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Move all tensor values in the inputs to GPU\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a391a3b-47f0-4d1b-964e-f891d97dfd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```python\u001b[31m\n",
      "import numpy as np\n",
      "import matplotlib\u001b[0m\u001b[32m.pyplot as plt\n",
      "\n",
      "# Calculate\u001b[0m_\u001b[34mthrough_\u001b[0m\u001b[35mthrough_\u001b[0m\u001b[31mthrough_\u001b[0m\u001b[32mthrough_through_through\u001b[0m\u001b[34m_through_through_through_through_\u001b[0m\u001b[35mthrough_through_through_through_through_through\u001b[0m\u001b[31m_through_through_mod\u001b[0m_\u001b[32mthrough_\u001b[0m!!_\u001b[34mthrough_\u001b[0m\u001b[35mthrough_through_through_through_through_through\u001b[0m\u001b[31m_through_through_through_through_through_\u001b[0m\u001b[32mthrough_through_through_through_through_through\u001b[0m\u001b[34m_through_through_through_through_through_\u001b[0m\u001b[35mthrough_through_through_through_through_through\u001b[0m!\t\t\t\t\t\t\t\t\t\t<\t\t\t\t\t\t\t\t\t\t</p>\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t_p!\u001b[31m!!\u001b[0m!\u001b[32m_on\u001b[0m_\u001b[34mthrough_\u001b[0m\u001b[35mthrough_through_through_through_through_through\u001b[0m\u001b[31m_through_through_through_through_through_\u001b[0m\u001b[32mthrough_through_through_through_through_through\u001b[0m\u001b[34m_through_through_through_through_through_\u001b[0m\u001b[35mthrough_through_through_through_through_through\u001b[0m\u001b[31m_through_through_through_through_through_\u001b[0m\u001b[32mthrough_through_through_through_through_through\u001b[0m\u001b[34m_through_through_through_through_through_\u001b[0m\u001b[35mthrough_through_through_through_through_through\u001b[0m\u001b[31m_through_through_through_through_through_\u001b[0m\u001b[32mthrough_through\u001b[0m\n",
      "Time taken:  2.6671017669141293\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "test_out = model.greedy_search_pld(inputs.input_ids,\n",
    "            attention_mask = inputs.attention_mask,\n",
    "            stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + 260)]),\n",
    "            prompt_matching_window_size = 3,\n",
    "            prompt_num_candidate_tokens = 50,\n",
    "            use_cache=True, \n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            print_output=True,\n",
    "            use_kv_compression=True\n",
    "        )\n",
    "end_time = time.perf_counter()\n",
    "print(\"\\nTime taken: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70151627-ffc4-410e-a051-a0b9582b9af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('delete', 0, 1, 0, 0), ('equal', 1, 4, 0, 3)]\n"
     ]
    }
   ],
   "source": [
    "# orig = [32013, 58, 24607, 60, 10587, 25, 10252, 11364, 185, 1892, 1181, 4016, 372, 21807, 185, 1892, 1575, 13371, 2875, 13, 4016, 13371, 372, 568, 83, 185, 185, 2, 24451, 387, 254, 5126, 185, 64, 3557, 62, 10432, 915, 405, 21807, 13, 18547, 7, 26701, 657, 62, 522, 62, 2781, 62, 3046, 8, 185, 4128, 7, 69, 1, 32, 3557, 13912, 915, 25, 507, 64, 3557, 62, 10432, 915, 92, 23792, 14, 2781, 2456, 185, 185, 2, 2284, 301, 1253, 254, 2441, 18596, 185, 449, 83, 13, 9707, 7, 26701, 657, 62, 522, 62, 2781, 62, 3046, 11, 30662, 28, 17, 15, 11, 3032, 28, 6, 10948, 1183, 5935, 3122, 28, 6, 8718, 1183, 23780, 28, 15, 13, 22, 8, 185, 449, 83, 13, 4740, 1497, 7194, 18596, 280, 13912, 915, 3588, 1152, 2462, 185, 449, 83, 13, 87, 1206, 1497, 31489, 657, 511, 11419, 2462, 185, 449, 83, 13, 88, 1206, 1497, 37, 18606, 2462, 185, 449, 83, 13, 1099, 85, 1027, 7, 64, 3557, 62, 10432, 915, 11, 3032, 28, 6, 504, 1183, 17128, 370, 1999, 28, 6, 28536, 1183, 1348, 2166, 28, 16, 8, 185, 449, 83, 13, 818, 7, 64, 3557, 62, 10432, 915, 9, 15, 13, 24, 11, 3034, 7, 449, 83, 13, 88, 3431, 4683, 9, 15, 13, 24, 11, 267, 6, 32, 3557, 25, 507, 64, 3557, 62, 10432, 915, 22587, 17, 69, 92, 1183, 3032, 405, 651, 504, 2462, 185, 449, 83, 13, 7304, 822, 185, 10252, 207, 185, 185, 24417, 25, 2744, 340, 4058, 2252, 1371, 11778, 276, 1328, 473, 207, 15, 207, 185, 185, 4851, 2030, 2974, 21926, 14, 24607, 60]\n",
    "# new = [185, 185, 10252, 11364, 185, 1892, 1181, 4016, 372, 21807, 185, 1892, 1575, 13371, 2875, 13, 4016, 13371, 372, 568, 83, 185, 185, 2, 24451, 387, 254, 5126, 185, 64, 3557, 62, 10432, 915, 405, 21807, 13, 18547, 7, 26701, 657, 62, 522, 62, 2781, 62, 3046, 8, 185, 4128, 7, 69, 1, 32, 3557, 13912, 915, 25, 507, 64, 3557, 62, 10432, 915, 92, 23792, 14, 2781, 2456, 185]\n",
    "# sm = difflib.SequenceMatcher(None, [1, 2, 3, 4], [2, 3, 4])\n",
    "# print(list(sm.get_opcodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd2621-2c2a-4eda-ac6c-f3cd87997131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
