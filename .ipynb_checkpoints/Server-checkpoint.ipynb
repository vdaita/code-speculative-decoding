{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e94e4e-8ca0-462a-9f41-95a2f41a944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers # requires transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1668d6c1-d608-4f91-8741-b33693e83d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.43.3\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab011d54-7e30-4fc1-a70c-f92d81737801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769ff7fbf4c74d70b12b564d49246eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "# model_name = \"TheBloke/deepseek-coder-6.7B-base-GPTQ\"\n",
    "# model_revision = \"gptq-4bit-32g-actorder_True\"\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "# draft_model_name = \"TheBloke/deepseek-coder-1.3b-base-GPTQ\"\n",
    "# model_revision =  \"gptq-4bit-32g-actorder_True\"\n",
    "draft_model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\")#, revision=model_revision)\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, trust_remote_code=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be64b3a4-aff3-4330-8e09-e2ff43823861",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd3b3c4-7eb8-47f2-9bec-87d0a12bf98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1202]\n",
      "[32013, 185]\n",
      "[32013, 1672]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"...\"))\n",
    "print(tokenizer.encode(\"\"\"\n",
    "\"\"\"))\n",
    "print(tokenizer.encode(\"##\"))\n",
    "\n",
    "newline_token = tokenizer.encode(\"\"\"\n",
    "\"\"\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88cde95-4062-4a4c-8ef2-146c693f3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            # if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "            #     return input_ids[0, start_idx:end_idx]\n",
    "            if start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:min(end_idx, input_length)]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([100], dtype=torch.long, device=input_ids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1d692b-10d3-487e-a782-f8dd57602ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Hunk:\n",
    "    filepath: str\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class SearchReplaceChange:\n",
    "    filepath: str\n",
    "    search_block: str\n",
    "    replace_block: str\n",
    "\n",
    "def find_hunks(diff_string):\n",
    "    hunks = []\n",
    "    current_filename = \"\"\n",
    "    current_lines = \"\"\n",
    "    for line in diff_string.splitlines():\n",
    "        if line.startswith(\"---\"):\n",
    "            continue\n",
    "        elif line.lstrip().startswith(\"+++\"):\n",
    "            if len(current_filename) > 0:\n",
    "                hunks.append(Hunk(current_filename, current_lines))\n",
    "            current_filename = line[3:]\n",
    "            current_lines = \"\"\n",
    "        elif line.lstrip().startswith(\"@@\"):\n",
    "            if len(current_filename) > 0:\n",
    "                hunks.append(Hunk(current_filename, current_lines))\n",
    "            current_lines = \"\"\n",
    "        else:\n",
    "            current_lines += line\n",
    "            current_lines += \"\\n\"\n",
    "    hunks.append(Hunk(current_filename, current_lines))\n",
    "    return hunks\n",
    "\n",
    "def parse_diff(diff_string):\n",
    "    hunks = find_hunks(diff_string)\n",
    "    search_replace_blocks = []\n",
    "\n",
    "    for hunk in hunks:\n",
    "        filepath = hunk.filepath\n",
    "        text = hunk.text\n",
    "\n",
    "        search_block = \"\"\n",
    "        replace_block = \"\"\n",
    "\n",
    "        for line in text.splitlines():\n",
    "            if line.startswith(\"-\"):\n",
    "                search_block += \" \" + line[1:] + \"\\n\"\n",
    "            elif line.startswith(\"+\"):\n",
    "                replace_block += \" \" + line[1:] + \"\\n\"\n",
    "            else:\n",
    "                search_block += line + \"\\n\"\n",
    "                replace_block += line + \"\\n\"\n",
    "\n",
    "        search_replace_blocks.append(\n",
    "            SearchReplaceChange(filepath, search_block, replace_block)\n",
    "        )\n",
    "        filepath = \"\"\n",
    "        search_block = \"\"\n",
    "        replace_block = \"\"\n",
    "\n",
    "    search_replace_blocks.append(\n",
    "        SearchReplaceChange(filepath, search_block, replace_block)\n",
    "    )\n",
    "\n",
    "    return search_replace_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c9ebba7-1c8a-4a4b-8956-fabeddce9fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "\n",
    "@dataclass\n",
    "class Match:\n",
    "    block: str\n",
    "    score: float\n",
    "\n",
    "def line_relevant(line):\n",
    "    return not(len(line.strip()) == 0 or line.startswith(\"#\") or line.startswith(\"//\"))\n",
    "\n",
    "def find_best_match(query_code: str, original_code: str):\n",
    "    query_code = query_code.strip()\n",
    "\n",
    "    original_lines = original_code.splitlines()\n",
    "    query_lines = query_code.splitlines()\n",
    "\n",
    "    if len(query_lines) == 0:\n",
    "        return Match(\"SUPERDOCSTHISSTRINGWILLNEVEREVERBEFOUND\", 100)\n",
    "\n",
    "    best_match = Match(\"\", -1)\n",
    "\n",
    "    for start_line in range(len(original_lines)):\n",
    "        min_end = min(len(original_lines), max(start_line, start_line + len(query_lines) - 5)) # +/- 5 lines for tolerance\n",
    "        max_end = min(len(original_lines), start_line + len(query_lines) + 5)\n",
    "        for end_line in range(min_end, max_end):\n",
    "            full_original_snippet = \"\\n\".join(original_lines[start_line:end_line + 1])\n",
    "\n",
    "            snippet_from_original = \"\\n\".join([line for line in original_lines[start_line:end_line + 1] if line_relevant(line)]) # the loop already doesn't include max_end\n",
    "            snippet_from_query = \"\\n\".join([line for line in query_lines if line_relevant(line)])\n",
    "\n",
    "            stripped_original = \" \".join([line.strip() for line in snippet_from_original.splitlines()])\n",
    "            stripped_query =  \" \".join([line.strip() for line in snippet_from_query.splitlines()])\n",
    "\n",
    "            score = fuzz.ratio(stripped_original, stripped_query)\n",
    "\n",
    "            # Weighting the first and last lines by 3x\n",
    "            score += 3*fuzz.ratio(original_lines[start_line], query_lines[0])\n",
    "            score += 3*fuzz.ratio(original_lines[end_line], query_lines[-1])\n",
    "\n",
    "            if score > best_match.score:\n",
    "                best_match = Match(full_original_snippet, score)\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5466bcc-865b-4333-9cae-4d3fa4afd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens_diff(input_ids, code_ids, orig_input_len=0, ngram_size=3, num_pred_tokens=10):\n",
    "    # start_time = time.perf_counter()\n",
    "    input_length = input_ids.size(1)\n",
    "    code_length = len(code_ids)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if ngram_size <= 0 or ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    sm = difflib.SequenceMatcher(None, code_ids, input_ids[0, orig_input_len:].tolist())\n",
    "    \n",
    "    deleted = added = changed = same = last_deleted = 0\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            changed += i2 - i1\n",
    "        elif tag == 'delete':\n",
    "            deleted += i2 - i1\n",
    "            last_deleted = i2 - i1\n",
    "        elif tag == 'insert':\n",
    "            added += j2 - j1\n",
    "        elif tag == 'equal':\n",
    "            same += i2 - i1\n",
    "    \n",
    "    approx_tokens_original = changed + deleted + same - last_deleted\n",
    "\n",
    "    lookback_start = max(input_length - ngram_size, orig_input_len)\n",
    "    search_ngram = input_ids[0, lookback_start:].tolist()\n",
    "\n",
    "    for ngram_start in range(max(0, approx_tokens_original - ngram_size), len(code_ids)):\n",
    "        # if there is a match, return the entire rest of the tokens.\n",
    "        if ngram_start + len(search_ngram) >= len(code_ids):\n",
    "            break\n",
    "        if search_ngram == code_ids[ngram_start:ngram_start + len(search_ngram)]:\n",
    "            return torch.tensor(code_ids[ngram_start + len(search_ngram):max(ngram_start + len(search_ngram) + num_pred_tokens, len(code_ids))], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "    # If no match is found, return what the answer would be otherwise\n",
    "    # print(\"Diff searching took: \", time.perf_counter() - start_time)\n",
    "    return find_candidate_pred_tokens(input_ids, ngram_size, num_pred_tokens)\n",
    "    # return torch.tensor([], dtype=torch.long, device=input_ids.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7bc8977-4805-48d6-b1af-d0be832876ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
    "UNDERLINE = \"\\x1b[4m\"\n",
    "RESET = \"\\x1b[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bca616bf-a9b7-4fe4-9276-86a311b5472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.candidate_generator import CandidateGenerator, _crop_past_key_values\n",
    "from transformers.generation.stopping_criteria import StoppingCriteria\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class DiffPromptLookupCandidateGenerator(CandidateGenerator):\n",
    "    def __init__(self, input_ids, code_ids, ngram_size=3, num_pred_tokens=10):\n",
    "        self.code_ids = code_ids\n",
    "        self.orig_input_len = input_ids.shape[-1]\n",
    "        self.ngram_size = ngram_size\n",
    "        self.num_pred_tokens = num_pred_tokens\n",
    "    \n",
    "    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n",
    "        # print(\"Getting candidates\")\n",
    "        return torch.cat(\n",
    "            (\n",
    "                input_ids,\n",
    "                find_candidate_pred_tokens_diff(input_ids, self.code_ids, self.orig_input_len, self.ngram_size, self.num_pred_tokens).unsqueeze(0)\n",
    "            ),\n",
    "            dim=-1\n",
    "        ), None\n",
    "    \n",
    "    def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int): # Maybe use the number of matches/scores to have a threshold\n",
    "        pass \n",
    "\n",
    "class NumRunsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, max_num_runs=4):\n",
    "        self.max_num_runs = 4\n",
    "        self.num_runs = 0\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
    "        self.num_runs += 1\n",
    "        return self.num_runs >= self.max_num_runs\n",
    "\n",
    "class NewlineStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, prompt_tokens: int, newline_count=5):\n",
    "        self.newline_token = tokenizer.encode(\"\"\"\n",
    "\"\"\")[-1]\n",
    "        self.newline_count = newline_count\n",
    "        self.prompt_tokens = prompt_tokens\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
    "        considered_tokens = input_ids[:, self.prompt_tokens:].list()\n",
    "        newline_list = [self.newline_token for i in range(self.newline_count)]\n",
    "        return newline_line in considered_tokens[0]\n",
    "\n",
    "def _get_default_candidate_generator_generator(generator: CandidateGenerator):\n",
    "    def _get_candidate_generator(self, **kwargs):\n",
    "        return generator\n",
    "    return _get_candidate_generator\n",
    "\n",
    "class TwoLayerLookupCandidateGenerator(CandidateGenerator):\n",
    "    def __init__(self, tokenizer, prompt_tokens, draft_model, input_ids, code_ids, num_runs=4, **diff_prompt_args):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_tokens = prompt_tokens\n",
    "        self.draft_model = draft_model\n",
    "        self.input_ids = input_ids\n",
    "        self.code_ids = code_ids\n",
    "        self.candidate_generator = DiffPromptLookupCandidateGenerator(\n",
    "            self.input_ids, \n",
    "            self.code_ids,\n",
    "            **diff_prompt_args\n",
    "        )\n",
    "        self.draft_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        self.past_keys_values = None\n",
    "        self.num_runs = num_runs\n",
    "\n",
    "        self.draft_model._get_candidate_generator = (_get_default_candidate_generator_generator(self.candidate_generator)).__get__(self.draft_model, type(self.draft_model))\n",
    "\n",
    "    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n",
    "        if self.past_keys_values:\n",
    "            self.past_keys_values = _crop_past_key_values(self.draft_model, self.past_keys_values, input_ids.shape[-1] - 1)\n",
    "\n",
    "        starting_input_length = input_ids.shape[-1]\n",
    "        # print(\"Getting draft candidates\")\n",
    "        \n",
    "        generation = self.draft_model.generate(\n",
    "            inputs=input_ids,\n",
    "            attention_mask=torch.ones(input_ids.shape[-1], device=input_ids.device).unsqueeze(0),\n",
    "            prompt_lookup_num_tokens=1,\n",
    "            max_new_tokens=1000,\n",
    "            stopping_criteria=[NumRunsStoppingCriteria(self.num_runs), NewlineStoppingCriteria(self.tokenizer, self.prompt_tokens)],\n",
    "            past_key_values=self.past_keys_values,\n",
    "            use_cache=True,\n",
    "            # output_logits=True,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "        return generation.sequences, None\n",
    "\n",
    "    def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int): # Maybe use the number of matches/scores to have a threshold\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3598b1e-8a2b-4aba-987c-898ee88ce477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1262226]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>## Code Before:\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "## Instruction:\n",
      "Add a \"sub\" function that subtracts two numbers. Also write docstrings for both functions and change a,b to x,y.\n",
      "## Code After:\n",
      "def add(x, y):\n",
      "    \"\"\"Adds two numbers.\"\"\"\n",
      "    return x + y\n",
      "\n",
      "def sub(x, y):\n",
      "    \"\"\"Subtracts two numbers.\"\"\"\n",
      "    return x - y\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Code Before:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Calculate the average\n",
      "average_throughput = np.mean(tokens_per_sec_arr)\n",
      "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
      "\n",
      "# Plotting the histogram\n",
      "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
      "plt.title('Histogram of Throughput Values')\n",
      "plt.xlabel('Tokens per Second')\n",
      "plt.ylabel('Frequency')\n",
      "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
      "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      "plt.show()\n",
      "\n",
      "## Instruction:\n",
      "Can you please change x axis to start from 0\n",
      "## Code After:\n",
      "import\n",
      "SEARCH\n",
      " \n",
      " import numpy as np\n",
      " import matplotlib.pyplot as plt\n",
      " \n",
      " # Calculate the average\n",
      " average_throughput = np.mean(tokens_per_sec_arr)\n",
      " print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
      " \n",
      " # Plotting the histogram\n",
      " plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
      " plt.title('Histogram of Throughput Values')\n",
      " plt.xlabel('Tokens per Second')\n",
      " plt.ylabel('Frequency')\n",
      " plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
      " plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      " plt.show()\n",
      "\n",
      "FOUND BLOCK\n",
      " import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Calculate the average\n",
      "average_throughput = np.mean(tokens_per_sec_arr)\n",
      "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
      "\n",
      "# Plotting the histogram\n",
      "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
      "plt.title('Histogram of Throughput Values')\n",
      "plt.xlabel('Tokens per Second')\n",
      "plt.ylabel('Frequency')\n",
      "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
      "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      "plt.show()\n",
      "REPLACE\n",
      " \n",
      " import\n",
      "\n",
      "INFO:     127.0.0.1:51820 - \"POST /edit_request HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Path\n",
    "from pydantic import BaseModel\n",
    "from typing import Annotated, Union\n",
    "import uvicorn\n",
    "import asyncio\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from transformers import TextStreamer\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class EditRequest(BaseModel):\n",
    "    file_content: str\n",
    "    query: str\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.post(\"/edit_request\")\n",
    "async def edit_request(edit_request: EditRequest):\n",
    "    # prompt = \"Code:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:\\n\".format(code_text=code_text, question=question)\n",
    "    shot = \"\"\"## Code Before:\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "## Instruction:\n",
    "Add a \"sub\" function that subtracts two numbers. Also write docstrings for both functions and change a,b to x,y.\n",
    "## Code After:\n",
    "def add(x, y):\n",
    "    \\\"\\\"\\\"Adds two numbers.\\\"\\\"\\\"\n",
    "    return x + y\n",
    "\n",
    "def sub(x, y):\n",
    "    \\\"\\\"\\\"Subtracts two numbers.\\\"\\\"\\\"\n",
    "    return x - y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = shot + \"\\n\" + f\"\"\"## Code Before:\n",
    "{edit_request.file_content}\n",
    "## Instruction:\n",
    "{edit_request.query}\n",
    "## Code After:\n",
    "\"\"\"\n",
    "    \n",
    "    code_inputs = tokenizer(edit_request.file_content, return_tensors=\"pt\").input_ids[0].tolist()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # inputs = tokenizer.apply_chat_template(\n",
    "    #     [\n",
    "    #         {\n",
    "    #             \"role\": \"user\",\n",
    "    #             \"content\": prompt\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"role\": \"assistant\",\n",
    "    #             \"content\": \"```\\n\"\n",
    "    #         }\n",
    "    #     ],\n",
    "    #     tokenize=True,\n",
    "    #     add_generation_prompt=True,\n",
    "    #     return_tensors=\"pt\"\n",
    "    # ).to(model.device)\n",
    "    \n",
    "    # Move all tensor values in the inputs to GPU\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "    # num_max_gen_tokens = inputs.shape[-1] + 300\n",
    "    num_max_gen_tokens = inputs.input_ids.shape[-1] + 300\n",
    "\n",
    "    # diff_candidate_generator = DiffPromptLookupCandidateGenerator(\n",
    "    #     inputs.input_ids, \n",
    "    #     code_inputs\n",
    "    # )\n",
    "    \n",
    "    # draft_model._get_candidate_generator = (_get_default_candidate_generator_generator(diff_candidate_generator)).__get__(draft_model, type(draft_model))\n",
    "    \n",
    "\n",
    "    two_layer_candidate_generator = TwoLayerLookupCandidateGenerator(\n",
    "        tokenizer,\n",
    "        inputs.input_ids.shape[-1],\n",
    "        draft_model,\n",
    "        inputs.input_ids,\n",
    "        code_inputs,\n",
    "        ngram_size=5,\n",
    "        num_pred_tokens=50\n",
    "    )\n",
    "    model._get_candidate_generator = (_get_default_candidate_generator_generator(two_layer_candidate_generator)).__get__(model, type(model))\n",
    "\n",
    "    test_out = model.generate(\n",
    "        inputs=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        prompt_lookup_num_tokens=1,\n",
    "        max_new_tokens=num_max_gen_tokens,\n",
    "        stopping_criteria=[NewlineStoppingCriteria(tokenizer, inputs.input_ids.shape[-1])],\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    \n",
    "    text = tokenizer.batch_decode(test_out[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0]\n",
    "    # text = tokenizer.batch_decode(test_out, skip_special_tokens=True)[0]\n",
    "    \n",
    "    unified_diff = \"\\n\".join(difflib.unified_diff(edit_request.file_content.splitlines(), text.splitlines(), n=3))\n",
    "    search_replace_changes = parse_diff(unified_diff)\n",
    "    # print(\"Search replace changes: \", search_replace_changes)\n",
    "    \n",
    "    fixed_file = edit_request.file_content\n",
    "    for sr in search_replace_changes:\n",
    "        if len(sr.search_block.strip()) == 0:\n",
    "            continue\n",
    "        print(\"SEARCH\\n\", sr.search_block)\n",
    "        sr.search_block = find_best_match(sr.search_block, edit_request.file_content).block\n",
    "        print(\"FOUND BLOCK\\n\", sr.search_block)\n",
    "        print(\"REPLACE\\n\", sr.replace_block)\n",
    "        fixed_file = fixed_file.replace(sr.search_block, f\"\"\"<<<<<<< SEARCH\n",
    "{sr.search_block}\n",
    "=======\n",
    "{sr.replace_block}\n",
    ">>>>>>> REPLACE\"\"\")\n",
    "\n",
    "    return {\"text\": fixed_file}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app)\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67534b8-7db7-49f1-8711-309d978bcaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
