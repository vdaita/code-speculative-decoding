{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e94e4e-8ca0-462a-9f41-95a2f41a944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers # requires transformers==4.35.2\n",
    "draft_device = torch.device('cuda:0')\n",
    "model_device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1668d6c1-d608-4f91-8741-b33693e83d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.43.3\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c100275b-9db8-432f-bbec-b88c715ac61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# draft_model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "# draft_model_name =\"codellama/CodeLlama-7b-hf\"\n",
    "draft_model_name = \"bigcode/starcoderbase-1b\"\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, trust_remote_code=True, torch_dtype=torch.float16, use_flash_attention_2=True).to(draft_device)#, load_in_4bit=True)\n",
    "print(draft_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e27a50-63e6-45c1-88c2-e43d2fa94bc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GPTBigCodeForCausalLM.__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigcode/starcoderbase\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(model_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/transformers/modeling_utils.py:3788\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3782\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3783\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3784\u001b[0m )\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3787\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3788\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3791\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[0;31mTypeError\u001b[0m: GPTBigCodeForCausalLM.__init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "# model_name = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "# model_name=\"codellama/CodeLlama-70b-hf\"\n",
    "model_name = \"bigcode/starcoderbase\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device=model_device, torch_dtype=torch.float16, use_flash_attention_2=True).to(model_device)#, load_in_4bit=True)#  , use_flash_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaeb3f8-710f-4813-81f4-df9386f21790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nuprl/CanItEdit\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929b62b-bb7d-41a8-9216-80907072ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            # if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "            #     return input_ids[0, start_idx:end_idx]\n",
    "            if start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:min(end_idx, input_length)]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([100], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens_diff(input_ids, code_ids, orig_input_len=0, ngram_size=3, num_pred_tokens=10):\n",
    "    # print(input_ids, code_ids)\n",
    "    \n",
    "    # start_time = time.perf_counter()\n",
    "    input_length = input_ids.size(1)\n",
    "    code_length = len(code_ids)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if ngram_size <= 0 or ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    sm = difflib.SequenceMatcher(None, code_ids, input_ids[0, orig_input_len:].tolist())\n",
    "    \n",
    "    deleted = added = changed = same = last_deleted = 0\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            changed += i2 - i1\n",
    "        elif tag == 'delete':\n",
    "            deleted += i2 - i1\n",
    "            last_deleted = i2 - i1\n",
    "        elif tag == 'insert':\n",
    "            added += j2 - j1\n",
    "        elif tag == 'equal':\n",
    "            same += i2 - i1\n",
    "    \n",
    "    approx_tokens_original = changed + deleted + same - last_deleted\n",
    "\n",
    "    lookback_start = max(input_length - ngram_size, orig_input_len)\n",
    "    search_ngram = input_ids[0, lookback_start:].tolist()\n",
    "\n",
    "    for ngram_start in range(max(0, approx_tokens_original - ngram_size), len(code_ids)):\n",
    "        # if there is a match, return the entire rest of the tokens.\n",
    "        if ngram_start + len(search_ngram) >= len(code_ids):\n",
    "            break\n",
    "        if search_ngram == code_ids[ngram_start:ngram_start + len(search_ngram)]:\n",
    "            return torch.tensor(code_ids[ngram_start + len(search_ngram):max(ngram_start + len(search_ngram) + num_pred_tokens, len(code_ids))], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "    # If no match is found, return what the answer would be otherwise\n",
    "    # print(\"Diff searching took: \", time.perf_counter() - start_time)\n",
    "    return find_candidate_pred_tokens(input_ids, ngram_size, num_pred_tokens)\n",
    "    # return torch.tensor([], dtype=torch.long, device=input_ids.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dfe19b-9c65-4420-856b-c3bdb3175f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.candidate_generator import CandidateGenerator, _crop_past_key_values\n",
    "from transformers.generation.stopping_criteria import StoppingCriteria\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from typing import Tuple, Optional\n",
    "import time\n",
    "\n",
    "class DiffPromptLookupCandidateGenerator(CandidateGenerator):\n",
    "    def __init__(self, input_ids, code_ids, ngram_size=3, num_pred_tokens=10):\n",
    "        self.code_ids = code_ids\n",
    "        self.orig_input_len = input_ids.shape[-1]\n",
    "        self.ngram_size = ngram_size\n",
    "        self.num_pred_tokens = num_pred_tokens\n",
    "        self.last_predicted = 0\n",
    "    \n",
    "    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n",
    "        # print(\"Getting candidates\")\n",
    "        new_tokens = find_candidate_pred_tokens_diff(input_ids, self.code_ids, self.orig_input_len, self.ngram_size, self.num_pred_tokens).unsqueeze(0)\n",
    "        self.last_predicted = new_tokens.shape[-1]\n",
    "        \n",
    "        return torch.cat(\n",
    "            (\n",
    "                input_ids,\n",
    "                new_tokens\n",
    "            ),\n",
    "            dim=-1\n",
    "        ), None\n",
    "    \n",
    "    def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int): # Maybe use the number of matches/scores to have a threshold\n",
    "        pass\n",
    "        # if num_matches == self.last_predicted:\n",
    "        #     self.num_pred_tokens *= 1.5\n",
    "        # else:\n",
    "        #     self.num_pred_tokens /= 1.5\n",
    "        # self.num_pred_tokens = int(self.num_pred_tokens)\n",
    "        # self.num_pred_tokens = min(self.num_pred_tokens, 100)\n",
    "        # self.num_pred_tokens = max(self.num_pred_tokens, 1)\n",
    "\n",
    "class NumRunsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, max_num_runs=4):\n",
    "        self.max_num_runs = 4\n",
    "        self.num_runs = 0\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
    "        self.num_runs += 1\n",
    "        return self.num_runs >= self.max_num_runs\n",
    "\n",
    "\n",
    "class CodeContentStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, prompt_tokens: int, newline_count=5):\n",
    "        self.newline_token = tokenizer.encode(\"\"\"\n",
    "\"\"\")[-1]\n",
    "        self.code_block_token = tokenizer.encode(\"```\")[-1]\n",
    "        # print(\"CODE CONTENT TOKEN: \", self.code_block_token)\n",
    "        \n",
    "        self.newline_count = newline_count\n",
    "        self.prompt_tokens = prompt_tokens\n",
    "        \n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
    "        considered_tokens = input_ids[:, self.prompt_tokens:][0]\n",
    "        return (self.code_block_token == considered_tokens).any().item()\n",
    "        # considered_tokens = tokenizer.batch_decode(input_ids[:, self.prompt_tokens:])[0]\n",
    "        # newline_list = \"\\n\"*self.newline_count\n",
    "        # print(newline_list, considered_tokens)\n",
    "        # return newline_list in considered_tokens\n",
    "\n",
    "class ScoreStoppingCriteria:\n",
    "    def __init__(self, min_score):\n",
    "        self.min_score = min_score\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
    "        if not(scores):\n",
    "            # print(\"No scores\")\n",
    "            return False\n",
    "        else:\n",
    "            ...\n",
    "            # print(\"Got scores scores stopping: \", scores[0].shape, len(scores))\n",
    "        scores_tensor = torch.stack(scores, dim=0)\n",
    "        softmax_scores = F.softmax(scores_tensor, 2)\n",
    "        # print(softmax_scores)\n",
    "        return (softmax_scores.max(dim=2).values < self.min_score).any().item()\n",
    "\n",
    "def _get_default_candidate_generator_generator(generator: CandidateGenerator):\n",
    "    def _get_candidate_generator(self, **kwargs):\n",
    "        return generator\n",
    "    return _get_candidate_generator\n",
    "\n",
    "class CodeTwoLayerLookupCandidateGenerator(CandidateGenerator):\n",
    "    def __init__(self, tokenizer, prompt_tokens, draft_model, input_ids, code_ids, use_score_check=False, min_score=0, scores_count=0, num_runs=4, **diff_prompt_args):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_tokens = prompt_tokens\n",
    "        self.draft_model = draft_model\n",
    "        self.input_ids = input_ids\n",
    "        self.code_ids = code_ids\n",
    "        self.candidate_generator = DiffPromptLookupCandidateGenerator(\n",
    "            self.input_ids, \n",
    "            self.code_ids,\n",
    "            **diff_prompt_args\n",
    "        )\n",
    "        self.draft_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        self.past_key_values = None\n",
    "        self.num_runs = num_runs\n",
    "\n",
    "        self.draft_model._get_candidate_generator = (_get_default_candidate_generator_generator(self.candidate_generator)).__get__(self.draft_model, type(self.draft_model))\n",
    "\n",
    "        self.start_token_index = self.input_ids.shape[-1]\n",
    "        self.min_score = min_score\n",
    "        self.scores_count = scores_count\n",
    "\n",
    "        self.use_score_check = use_score_check\n",
    "    \n",
    "    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n",
    "        if self.past_key_values:\n",
    "            self.past_key_values = _crop_past_key_values(self.draft_model, self.past_key_values, input_ids.shape[-1] - 1)\n",
    "\n",
    "        stopping_criteria = [NumRunsStoppingCriteria(self.num_runs), \n",
    "                            CodeContentStoppingCriteria(self.tokenizer, self.prompt_tokens), \n",
    "                            ]\n",
    "        if self.use_score_check:\n",
    "            stopping_criteria = [NumRunsStoppingCriteria(self.num_runs), \n",
    "                                 CodeContentStoppingCriteria(self.tokenizer, self.prompt_tokens), \n",
    "                                 ScoreStoppingCriteria(self.min_score)\n",
    "                                ]\n",
    "\n",
    "        # if self.past_key_values:\n",
    "        #     print(self.past_key_values[0][0].shape)\n",
    "\n",
    "        if self.past_key_values: \n",
    "            generation = self.draft_model.generate(\n",
    "                inputs=input_ids,\n",
    "                attention_mask=torch.ones(input_ids.shape[-1], device=input_ids.device).unsqueeze(0),\n",
    "                prompt_lookup_num_tokens=1,\n",
    "                max_new_tokens=1000,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                past_key_values=self.past_key_values,\n",
    "                use_cache=True,\n",
    "                # output_logits=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "        else:\n",
    "            generation = self.draft_model.generate(\n",
    "                inputs=input_ids,\n",
    "                attention_mask=torch.ones(input_ids.shape[-1], device=input_ids.device).unsqueeze(0),\n",
    "                prompt_lookup_num_tokens=1,\n",
    "                max_new_tokens=1000,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                use_cache=True,\n",
    "                # output_logits=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "        # print(\"Scores: \", generation.scores)\n",
    "\n",
    "        self.pred_tokens_count = generation.sequences.shape[-1] - input_ids.shape[-1]\n",
    "        self.past_key_values = generation.past_key_values\n",
    "        self.past_top_scores = torch.stack(generation.scores, dim=1).max(dim=1).values[0]\n",
    "\n",
    "        return generation.sequences, torch.stack(generation.scores, dim=1)\n",
    "\n",
    "    def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int):\n",
    "        if num_matches == self.pred_tokens_count:\n",
    "            if self.scores_count == 0:\n",
    "                self.min_score = 0\n",
    "            else:\n",
    "                self.min_score = (self.scores_count / self.scores_count + 1) * (self.min_score)\n",
    "        else:\n",
    "            if self.scores_count == 0:\n",
    "                self.min_score = self.past_top_scores[-num_matches]\n",
    "            else:\n",
    "                self.min_score = (self.scores_count / (self.scores_count + 1)) * (self.min_score) + (1 / (self.scores_count + 1)) * (self.past_top_scores[-1])\n",
    "        self.scores_count += 1\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c46248-828d-4d70-9dc2-41fc74d036ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_update(dictionary):\n",
    "    for key in dictionary:\n",
    "        print(\"\\t\", key, \": \", dictionary[key][-1])\n",
    "    print(\"======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a969129-dfd9-4c64-a164-6406f41c32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import TextStreamer\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "\n",
    "# lookup_tokens = [10, 20, 40, 60]\n",
    "# lookup_tokens = [40]\n",
    "lookup_tokens = [100, 120]\n",
    "stats = {lt: {\"method\": [], \"method_with_score_cutoff\": [], \"assisted\": [0], \"pld\": [], \"regular\": [0], \"lev_similarity\": [0], \"generated_tokens\": [0]} for lt in lookup_tokens}\n",
    "\n",
    "global_min_score = 0\n",
    "global_scores_count = 0\n",
    "\n",
    "regular_get_candidate_generator = model._get_candidate_generator\n",
    "\n",
    "for lt in lookup_tokens:\n",
    "    for row in tqdm(ds):\n",
    "        # input_text = \"## Code Before:\\n{code_text}\\n## Change requested: {question}\\n## Rewrite the code to incorporate the change.\\n\".format(code_text=row['before'], question=row['instruction_descriptive'])\n",
    "        # inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    #     inputs = tokenizer.apply_chat_template([\n",
    "    #         {\n",
    "    #             \"role\": \"user\",\n",
    "    #             \"content\": input_text\n",
    "    #         },\n",
    "    #     ], tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    #     response_prompt = tokenizer.encode(\"\"\"Sure, here is the modified code:\n",
    "    \n",
    "    # ```python\n",
    "    # \"\"\", return_tensors=\"pt\").to(model.device)[:, 1:]\n",
    "        # inputs = torch.cat((inputs, response_prompt), dim=-1)\n",
    "\n",
    "        input_text = f\"<commit_before>\\n{row['before']}\\n<commit_msg>\\n{row['instruction_descriptive']}\\n<commit_after>\\n\"\n",
    "        # input_text = f\"## Code Before:\\n{row['before']}\\n## Change Requested:\\n{row['instruction_descriptive']}\\n## Code After:\\n\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "        code_tokens = tokenizer(row['before'], return_tensors=\"pt\")\n",
    "        starting_input_tokens = inputs.shape[-1]\n",
    "        \n",
    "        max_new_tokens = code_tokens.input_ids.shape[-1] + 500\n",
    "\n",
    "        model._get_candidate_generator = (regular_get_candidate_generator).__get__(model, type(model))\n",
    "\n",
    "        # Use HuggingFace assisted decoding\n",
    "        # start_time = time.perf_counter()\n",
    "        # assisted_output = model.generate(\n",
    "        #     input_ids=inputs,\n",
    "        #     max_new_tokens=max_new_tokens,\n",
    "        #     stopping_criteria=[CodeContentStoppingCriteria(tokenizer, inputs.shape[-1])],\n",
    "        #     return_dict_in_generate=True,\n",
    "        #     output_scores=True,\n",
    "        #     assistant_model=draft_model\n",
    "        # )\n",
    "        # end_time = time.perf_counter()\n",
    "        # stats[lt][\"assisted\"].append(end_time - start_time)\n",
    "    \n",
    "        # # Use HuggingFace prompt lookup decoding\n",
    "        start_time = time.perf_counter()\n",
    "        pld_output = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=[CodeContentStoppingCriteria(tokenizer, inputs.shape[-1])],\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            prompt_lookup_num_tokens=lt\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "        stats[lt][\"pld\"].append(end_time - start_time)\n",
    "    \n",
    "        # # Use regular HuggingFace text generation\n",
    "        # start_time = time.perf_counter()\n",
    "        # regular_outputs = model.generate(\n",
    "        #     input_ids=inputs,\n",
    "        #     max_new_tokens=max_new_tokens,\n",
    "        #     stopping_criteria=[CodeContentStoppingCriteria(tokenizer, inputs.shape[-1])],\n",
    "        #     return_dict_in_generate=True,\n",
    "        #     output_scores=True\n",
    "        # )\n",
    "        # end_time = time.perf_counter()\n",
    "    \n",
    "        # stats[lt][\"regular\"].append(end_time - start_time)\n",
    "        # new_text = tokenizer.batch_decode(pld_output.sequences[:, starting_input_tokens:])[0]\n",
    "\n",
    "        # print(row['before'], new_text)\n",
    "    \n",
    "        # lev_similarity = Levenshtein.normalized_similarity(row['before'], new_text) \n",
    "        # stats[lt][\"lev_similarity\"].append(lev_similarity)\n",
    "    \n",
    "        # stats[lt][\"generated_tokens\"].append(pld_output.sequences.shape[-1])\n",
    "\n",
    "        # Two Layer Lookup Candidate Generator with Score Check\n",
    "        two_layer_candidate_generator = CodeTwoLayerLookupCandidateGenerator(\n",
    "            tokenizer,\n",
    "            inputs.shape[-1],\n",
    "            draft_model,\n",
    "            inputs,\n",
    "            code_tokens.input_ids.tolist()[0],\n",
    "            use_score_check=True,\n",
    "            min_score=global_min_score,\n",
    "            scores_count=global_scores_count,\n",
    "            ngram_size=5,\n",
    "            num_pred_tokens=lt\n",
    "        )\n",
    "        model._get_candidate_generator = (_get_default_candidate_generator_generator(two_layer_candidate_generator)).__get__(model, type(model))\n",
    "    \n",
    "        global_min_score = two_layer_candidate_generator.min_score\n",
    "        global_scores_count = two_layer_candidate_generator.scores_count\n",
    "        start_time = time.perf_counter()\n",
    "        test_out = model.generate(\n",
    "            inputs=inputs,\n",
    "            prompt_lookup_num_tokens=1,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=[CodeContentStoppingCriteria(tokenizer, inputs.shape[-1])],\n",
    "            use_cache=True,\n",
    "            # streamer=TextStreamer(tokenizer)\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "        stats[lt][\"method_with_score_cutoff\"].append(end_time - start_time)\n",
    "\n",
    "        # Two Layer Lookup Candidate Generator without Score Check\n",
    "        two_layer_candidate_generator = CodeTwoLayerLookupCandidateGenerator(\n",
    "            tokenizer,\n",
    "            inputs.shape[-1],\n",
    "            draft_model,\n",
    "            inputs,\n",
    "            code_tokens.input_ids.tolist()[0],\n",
    "            use_score_check=False,\n",
    "            min_score=global_min_score,\n",
    "            scores_count=global_scores_count,\n",
    "            ngram_size=5,\n",
    "            num_pred_tokens=lt\n",
    "        )\n",
    "        model._get_candidate_generator = (_get_default_candidate_generator_generator(two_layer_candidate_generator)).__get__(model, type(model))\n",
    "    \n",
    "        global_min_score = two_layer_candidate_generator.min_score\n",
    "        global_scores_count = two_layer_candidate_generator.scores_count\n",
    "        start_time = time.perf_counter()\n",
    "        test_out = model.generate(\n",
    "            inputs=inputs,\n",
    "            prompt_lookup_num_tokens=1,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=[CodeContentStoppingCriteria(tokenizer, inputs.shape[-1])],\n",
    "            use_cache=True,\n",
    "            # streamer=TextStreamer(tokenizer)\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "        stats[lt][\"method\"].append(end_time - start_time)\n",
    "\n",
    "        print_update(stats[lt])\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b627055-dbb8-472e-9cac-324a96fbf56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "stats_file = open(\"stats_starcoder_no_score_check_100_120.json\", \"w+\")\n",
    "stats_file.write(json.dumps(stats))\n",
    "stats_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb568c4c-d751-4067-8921-3cf6d3e2d3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
