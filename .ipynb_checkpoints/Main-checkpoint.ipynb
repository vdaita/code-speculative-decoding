{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8c6d79-33af-4e0d-8082-31d1ed2c4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "from transformers.generation.utils import _crop_past_key_values\n",
    "import difflib\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c8d9242-6460-4f27-8f97-4724465caa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a471c8d389c403bb5e6b85f896aa66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e665a82-6515-4f44-a2c1-445eb6c35bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32013,     2, 17437, 12606,  6656,   305,  2711,  6905,   279,  9942,\n",
      "           185,   185,  1551, 12606,     7,    77,  1772,   185,   315,   245,\n",
      "            11,   270,  1412,    15,    11,   207,    16,   185,   315,  1470,\n",
      "           245,  1013,   291,    25,   185,   436,  3628,     7,    64,    11,\n",
      "          1223,    28,     6,   651,     8,   185,   436,   245,    11,   270]])\n",
      "[\"<｜begin▁of▁sentence｜># Write fibbonacci sequence in python\\n\\ndef fib(n):\\n    a, b = 0, 1\\n    while a < n:\\n        print(a, end=' ')\\n        a, b\"]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer(\"# Write fibbonacci sequence in python\", return_tensors=\"pt\")\n",
    "prediction = target_model.generate(input_ids=token_ids.input_ids, max_new_tokens=40)\n",
    "print(prediction)\n",
    "print(tokenizer.batch_decode(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "efb10730-87cb-411c-af25-6e84b16bf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def regular_generate(target_model, tokenizer, input_str, max_seq_len=50):\n",
    "    input_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids\n",
    "    current_position = input_ids.shape[1]\n",
    "    token_ids = torch.zeros((1, max_seq_len), dtype=torch.int32)\n",
    "    token_ids[:, :current_position] = input_ids\n",
    "    \n",
    "    prediction = target_model(input_ids=token_ids[:, :current_position], use_cache=True, return_dict=True)\n",
    "    cache = prediction.past_key_values\n",
    "    pred_logits = prediction.logits\n",
    "    pred_logits = F.softmax(pred_logits, dim=-1)\n",
    "    token_ids[:, current_position] = torch.argmax(pred_logits[:, current_position - 1], dim=-1)\n",
    "    current_position += 1\n",
    "\n",
    "    while len(token_ids) < max_seq_len:\n",
    "        prediction = target_model(input_ids=token_ids[:, :current_position], use_cache=True, past_key_values=cache, return_dict=True, dtype=torch.float32)\n",
    "        cache = prediction.past_key_values\n",
    "        pred_logits = prediction.logits\n",
    "        pred_logits = F.softmax(pred_logits, dim=-1)\n",
    "        token_ids[:, current_position] = torch.argmax(pred_logits[:, current_position - 1], dim=-1)\n",
    "        print(tokenizer.batch_decode(token_ids[:, :current_position]))\n",
    "        current_position += 1\n",
    "        if token_ids[0, -1] == tokenizer.eos_token or token_ids[0, -1] == tokenizer.pad_token: # Check for newline\n",
    "            break\n",
    "\n",
    "    # print(tokenizer.batch_decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f653c1e6-8cc2-4149-9b3d-909f314bc8c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (14) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [14]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mregular_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m# Write a python function that generates the fibonacci sequence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 20\u001b[0m, in \u001b[0;36mregular_generate\u001b[0;34m(target_model, tokenizer, input_str, max_seq_len)\u001b[0m\n\u001b[1;32m     18\u001b[0m pred_logits \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     19\u001b[0m pred_logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(pred_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtoken_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_position\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(pred_logits[:, :current_position \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(token_ids[:, :current_position]))\n\u001b[1;32m     22\u001b[0m current_position \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (14) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [14]"
     ]
    }
   ],
   "source": [
    "regular_generate(target_model, tokenizer, \"# Write a python function that generates the fibonacci sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e4030-7339-456a-a617-9de8ca00631f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
