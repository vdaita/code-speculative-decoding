{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e94e4e-8ca0-462a-9f41-95a2f41a944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import transformers # requires transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1668d6c1-d608-4f91-8741-b33693e83d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.43.3\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab011d54-7e30-4fc1-a70c-f92d81737801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30d5ff0031240588ad24f13ca160525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "# model_name = \"TheBloke/deepseek-coder-6.7B-base-GPTQ\"\n",
    "# model_revision = \"gptq-4bit-32g-actorder_True\"\n",
    "model_name = \"nuprl/EditCoder-Multi-6.7b-v1\"\n",
    "\n",
    "# draft_model_name = \"TheBloke/deepseek-coder-1.3b-base-GPTQ\"\n",
    "# model_revision =  \"gptq-4bit-32g-actorder_True\"\n",
    "draft_model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)#, revision=model_revision)\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, trust_remote_code=True, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)#, revision=model_revision)\n",
    "\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "draft_model.forward = torch.compile(draft_model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "# model.generation_config.cache_implementation = \"static\"\n",
    "# draft_model.generation_config.cache_implementation = \"static\"\n",
    "\n",
    "# model = model.to_bettertransformer()\n",
    "# draft_model = draft_model.to_bettertransformer(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be64b3a4-aff3-4330-8e09-e2ff43823861",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd3b3c4-7eb8-47f2-9bec-87d0a12bf98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1202]\n",
      "[32013, 185]\n",
      "[32013, 1672]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"...\"))\n",
    "print(tokenizer.encode(\"\"\"\n",
    "\"\"\"))\n",
    "print(tokenizer.encode(\"##\"))\n",
    "\n",
    "newline_token = tokenizer.encode(\"\"\"\n",
    "\"\"\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88cde95-4062-4a4c-8ef2-146c693f3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            # if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "            #     return input_ids[0, start_idx:end_idx]\n",
    "            if start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:min(end_idx, input_length)]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([100], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens_diff(input_ids, code_ids, orig_input_len=0, ngram_size=3, num_pred_tokens=10):\n",
    "    # print(input_ids, code_ids)\n",
    "    \n",
    "    # start_time = time.perf_counter()\n",
    "    input_length = input_ids.size(1)\n",
    "    code_length = len(code_ids)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if ngram_size <= 0 or ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    sm = difflib.SequenceMatcher(None, code_ids, input_ids[0, orig_input_len:].tolist())\n",
    "    \n",
    "    deleted = added = changed = same = last_deleted = 0\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            changed += i2 - i1\n",
    "        elif tag == 'delete':\n",
    "            deleted += i2 - i1\n",
    "            last_deleted = i2 - i1\n",
    "        elif tag == 'insert':\n",
    "            added += j2 - j1\n",
    "        elif tag == 'equal':\n",
    "            same += i2 - i1\n",
    "    \n",
    "    approx_tokens_original = changed + deleted + same - last_deleted\n",
    "\n",
    "    lookback_start = max(input_length - ngram_size, orig_input_len)\n",
    "    search_ngram = input_ids[0, lookback_start:].tolist()\n",
    "\n",
    "    for ngram_start in range(max(0, approx_tokens_original - ngram_size), len(code_ids)):\n",
    "        # if there is a match, return the entire rest of the tokens.\n",
    "        if ngram_start + len(search_ngram) >= len(code_ids):\n",
    "            break\n",
    "        if search_ngram == code_ids[ngram_start:ngram_start + len(search_ngram)]:\n",
    "            return torch.tensor(code_ids[ngram_start + len(search_ngram):max(ngram_start + len(search_ngram) + num_pred_tokens, len(code_ids))], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "    # If no match is found, return what the answer would be otherwise\n",
    "    # print(\"Diff searching took: \", time.perf_counter() - start_time)\n",
    "    return find_candidate_pred_tokens(input_ids, ngram_size, num_pred_tokens)\n",
    "    # return torch.tensor([], dtype=torch.long, device=input_ids.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1d692b-10d3-487e-a782-f8dd57602ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Hunk:\n",
    "    filepath: str\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class SearchReplaceChange:\n",
    "    filepath: str\n",
    "    search_block: str\n",
    "    replace_block: str\n",
    "\n",
    "def find_hunks(diff_string):\n",
    "    hunks = []\n",
    "    current_filename = \"\"\n",
    "    current_lines = \"\"\n",
    "    for line in diff_string.splitlines():\n",
    "        if line.startswith(\"---\"):\n",
    "            continue\n",
    "        elif line.lstrip().startswith(\"+++\"):\n",
    "            if len(current_filename) > 0:\n",
    "                hunks.append(Hunk(current_filename, current_lines))\n",
    "            current_filename = line[3:]\n",
    "            current_lines = \"\"\n",
    "        elif line.lstrip().startswith(\"@@\"):\n",
    "            if len(current_filename) > 0:\n",
    "                hunks.append(Hunk(current_filename, current_lines))\n",
    "            current_lines = \"\"\n",
    "        else:\n",
    "            current_lines += line\n",
    "            current_lines += \"\\n\"\n",
    "    hunks.append(Hunk(current_filename, current_lines))\n",
    "    return hunks\n",
    "\n",
    "def parse_diff(diff_string):\n",
    "    hunks = find_hunks(diff_string)\n",
    "    search_replace_blocks = []\n",
    "\n",
    "    for hunk in hunks:\n",
    "        filepath = hunk.filepath\n",
    "        text = hunk.text\n",
    "\n",
    "        search_block = \"\"\n",
    "        replace_block = \"\"\n",
    "\n",
    "        for line in text.splitlines():\n",
    "            if line.startswith(\"-\"):\n",
    "                search_block += line[1:] + \"\\n\"\n",
    "            elif line.startswith(\"+\"):\n",
    "                replace_block += line[1:] + \"\\n\"\n",
    "            else:\n",
    "                search_block += line[1:] + \"\\n\"\n",
    "                replace_block += line[1:] + \"\\n\"\n",
    "\n",
    "        search_replace_blocks.append(\n",
    "            SearchReplaceChange(filepath, search_block, replace_block)\n",
    "        )\n",
    "        filepath = \"\"\n",
    "        search_block = \"\"\n",
    "        replace_block = \"\"\n",
    "\n",
    "    search_replace_blocks.append(\n",
    "        SearchReplaceChange(filepath, search_block, replace_block)\n",
    "    )\n",
    "\n",
    "    return search_replace_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c9ebba7-1c8a-4a4b-8956-fabeddce9fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "\n",
    "@dataclass\n",
    "class Match:\n",
    "    block: str\n",
    "    score: float\n",
    "\n",
    "def line_relevant(line):\n",
    "    return not(len(line.strip()) == 0 or line.startswith(\"#\") or line.startswith(\"//\"))\n",
    "\n",
    "def find_best_match(query_code: str, original_code: str):\n",
    "    query_code = query_code.strip()\n",
    "\n",
    "    original_lines = original_code.splitlines()\n",
    "    query_lines = query_code.splitlines()\n",
    "\n",
    "    if len(query_lines) == 0:\n",
    "        return Match(\"SUPERDOCSTHISSTRINGWILLNEVEREVERBEFOUND\", 100)\n",
    "\n",
    "    best_match = Match(\"\", -1)\n",
    "\n",
    "    for start_line in range(len(original_lines)):\n",
    "        min_end = min(len(original_lines), max(start_line, start_line + len(query_lines) - 5)) # +/- 5 lines for tolerance\n",
    "        max_end = min(len(original_lines), start_line + len(query_lines) + 5)\n",
    "        for end_line in range(min_end, max_end):\n",
    "            full_original_snippet = \"\\n\".join(original_lines[start_line:end_line + 1])\n",
    "\n",
    "            snippet_from_original = \"\\n\".join([line for line in original_lines[start_line:end_line + 1] if line_relevant(line)]) # the loop already doesn't include max_end\n",
    "            snippet_from_query = \"\\n\".join([line for line in query_lines if line_relevant(line)])\n",
    "\n",
    "            stripped_original = \" \".join([line.strip() for line in snippet_from_original.splitlines()])\n",
    "            stripped_query =  \" \".join([line.strip() for line in snippet_from_query.splitlines()])\n",
    "\n",
    "            score = fuzz.ratio(stripped_original, stripped_query)\n",
    "\n",
    "            # Weighting the first and last lines by 3x\n",
    "            score += 3*fuzz.ratio(original_lines[start_line], query_lines[0])\n",
    "            score += 3*fuzz.ratio(original_lines[end_line], query_lines[-1])\n",
    "\n",
    "            if score > best_match.score:\n",
    "                best_match = Match(full_original_snippet, score)\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7bc8977-4805-48d6-b1af-d0be832876ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
    "UNDERLINE = \"\\x1b[4m\"\n",
    "RESET = \"\\x1b[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bca616bf-a9b7-4fe4-9276-86a311b5472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.candidate_generator import CandidateGenerator, _crop_past_key_values\n",
    "from transformers.generation.stopping_criteria import StoppingCriteria, StopStringCriteria\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from typing import Tuple, Optional\n",
    "import time\n",
    "\n",
    "class DiffPromptLookupCandidateGenerator(CandidateGenerator):\n",
    "    def __init__(self, input_ids, code_ids, ngram_size=3, num_pred_tokens=10):\n",
    "        self.code_ids = code_ids\n",
    "        self.orig_input_len = input_ids.shape[-1]\n",
    "        self.ngram_size = ngram_size\n",
    "        self.num_pred_tokens = num_pred_tokens\n",
    "        self.last_predicted = 0\n",
    "    \n",
    "    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n",
    "        # print(\"Getting candidates\")\n",
    "        new_tokens = find_candidate_pred_tokens_diff(input_ids, self.code_ids, self.orig_input_len, self.ngram_size, self.num_pred_tokens).unsqueeze(0)\n",
    "        self.last_predicted = new_tokens.shape[-1]\n",
    "        \n",
    "        return torch.cat(\n",
    "            (\n",
    "                input_ids,\n",
    "                new_tokens\n",
    "            ),\n",
    "            dim=-1\n",
    "        ), None\n",
    "    \n",
    "    def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int): # Maybe use the number of matches/scores to have a threshold\n",
    "        pass\n",
    "        # if num_matches == self.last_predicted:\n",
    "        #     self.num_pred_tokens *= 1.5\n",
    "        # else:\n",
    "        #     self.num_pred_tokens /= 1.5\n",
    "        # self.num_pred_tokens = int(self.num_pred_tokens)\n",
    "        # self.num_pred_tokens = min(self.num_pred_tokens, 100)\n",
    "        # self.num_pred_tokens = max(self.num_pred_tokens, 1)\n",
    "\n",
    "class NumRunsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, max_num_runs=4):\n",
    "        self.max_num_runs = 4\n",
    "        self.num_runs = 0\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
    "        self.num_runs += 1\n",
    "        return self.num_runs >= self.max_num_runs\n",
    "\n",
    "class ScoreStoppingCriteria:\n",
    "    def __init__(self, min_score):\n",
    "        self.min_score = min_score\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
    "        if not(scores):\n",
    "            # print(\"No scores\")\n",
    "            return False\n",
    "        else:\n",
    "            ...\n",
    "            # print(\"Got scores scores stopping: \", scores[0].shape, len(scores))\n",
    "        scores_tensor = torch.stack(scores, dim=0)\n",
    "        softmax_scores = F.softmax(scores_tensor, 2)\n",
    "        # print(softmax_scores)\n",
    "        return (softmax_scores.max(dim=2).values < self.min_score).any().item()\n",
    "\n",
    "def _get_default_candidate_generator_generator(generator: CandidateGenerator):\n",
    "    def _get_candidate_generator(self, **kwargs):\n",
    "        return generator\n",
    "    return _get_candidate_generator\n",
    "\n",
    "class CodeTwoLayerLookupCandidateGenerator(CandidateGenerator):\n",
    "    def __init__(self, tokenizer, prompt_tokens, draft_model, input_ids, code_ids, use_score_check=False, min_score=0, scores_count=0, num_runs=4, **diff_prompt_args):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_tokens = prompt_tokens\n",
    "        self.draft_model = draft_model\n",
    "        self.input_ids = input_ids\n",
    "        self.code_ids = code_ids\n",
    "        self.candidate_generator = DiffPromptLookupCandidateGenerator(\n",
    "            self.input_ids, \n",
    "            self.code_ids,\n",
    "            **diff_prompt_args\n",
    "        )\n",
    "        self.draft_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        self.past_key_values = None\n",
    "        self.num_runs = num_runs\n",
    "\n",
    "        self.draft_model._get_candidate_generator = (_get_default_candidate_generator_generator(self.candidate_generator)).__get__(self.draft_model, type(self.draft_model))\n",
    "\n",
    "        self.start_token_index = self.input_ids.shape[-1]\n",
    "        self.min_score = min_score\n",
    "        self.scores_count = scores_count\n",
    "\n",
    "        self.use_score_check = use_score_check\n",
    "    \n",
    "    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n",
    "        if self.past_key_values:\n",
    "            self.past_key_values = _crop_past_key_values(self.draft_model, self.past_key_values, input_ids.shape[-1] - 1)\n",
    "\n",
    "        stopping_criteria = [NumRunsStoppingCriteria(self.num_runs), \n",
    "                            StopStringCriteria(self.tokenizer, [\"\\n\\n\\n\\n\\n\"]), \n",
    "                            ]\n",
    "        if self.use_score_check:\n",
    "            stopping_criteria = [NumRunsStoppingCriteria(self.num_runs), \n",
    "                                 StopStringCriteria(self.tokenizer, [\"\\n\\n\\n\\n\\n\"]), \n",
    "                                 ScoreStoppingCriteria(self.min_score)\n",
    "                                ]\n",
    "\n",
    "        # if self.past_key_values:\n",
    "        #     print(self.past_key_values[0][0].shape)\n",
    "\n",
    "        input_ids = input_ids.to(self.draft_model.device)\n",
    "\n",
    "        if self.past_key_values: \n",
    "            generation = self.draft_model.generate(\n",
    "                inputs=input_ids,\n",
    "                attention_mask=torch.ones(input_ids.shape[-1], device=input_ids.device).unsqueeze(0),\n",
    "                prompt_lookup_num_tokens=1,\n",
    "                max_new_tokens=1000,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                past_key_values=self.past_key_values,\n",
    "                use_cache=True,\n",
    "                # output_logits=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "        else:\n",
    "            generation = self.draft_model.generate(\n",
    "                inputs=input_ids,\n",
    "                attention_mask=torch.ones(input_ids.shape[-1], device=input_ids.device).unsqueeze(0),\n",
    "                prompt_lookup_num_tokens=1,\n",
    "                max_new_tokens=1000,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                use_cache=True,\n",
    "                # output_logits=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        # print(\"Scores: \", generation.scores)\n",
    "\n",
    "        self.pred_tokens_count = generation.sequences.shape[-1] - input_ids.shape[-1]\n",
    "        self.past_key_values = generation.past_key_values\n",
    "        self.past_top_scores = torch.stack(generation.scores, dim=1).max(dim=1).values[0]\n",
    "\n",
    "        return generation.sequences, torch.stack(generation.scores, dim=1)\n",
    "\n",
    "    def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int):\n",
    "        if num_matches == self.pred_tokens_count:\n",
    "            if self.scores_count == 0:\n",
    "                self.min_score = 0\n",
    "            else:\n",
    "                self.min_score = (self.scores_count / self.scores_count + 1) * (self.min_score)\n",
    "        else:\n",
    "            if self.scores_count == 0:\n",
    "                self.min_score = self.past_top_scores[-num_matches]\n",
    "            else:\n",
    "                self.min_score = (self.scores_count / (self.scores_count + 1)) * (self.min_score) + (1 / (self.scores_count + 1)) * (self.past_top_scores[-1])\n",
    "        self.scores_count += 1\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d617f6-c9d8-44e4-b9fb-c897c227fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3598b1e-8a2b-4aba-987c-898ee88ce477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [2380180]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>## Code Before:\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "## Instruction:\n",
      "Add a \"sub\" function that subtracts two numbers. Also write docstrings for both functions and change a,b to x,y.\n",
      "## Code After:\n",
      "def add(x, y):\n",
      "    \"\"\"Adds two numbers.\"\"\"\n",
      "    return x + y\n",
      "\n",
      "def sub(x, y):\n",
      "    \"\"\"Subtracts two numbers.\"\"\"\n",
      "    return x - y\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Code Before:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Calculate the average\n",
      "average_throughput = np.mean(tokens_per_sec_arr)\n",
      "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
      "\n",
      "# Plotting the histogram\n",
      "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
      "plt.title('Histogram of Throughput Values')\n",
      "plt.xlabel('Tokens per Second')\n",
      "plt.ylabel('Frequency')\n",
      "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
      "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      "plt.show()\n",
      "\n",
      "## Instruction:\n",
      "Can you please change x axis to start from 0\n",
      "## Code After:\n",
      "INFO:     127.0.0.1:57484 - \"POST /edit_request HTTP/1.1\" 500 Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/routing.py\", line 756, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/routing.py\", line 776, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/routing.py\", line 297, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/starlette/routing.py\", line 72, in app\n",
      "    response = await func(request)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/fastapi/routing.py\", line 278, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/tmp/ipykernel_2380180/811197843.py\", line 105, in edit_request\n",
      "    test_out = model.generate(\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jiawei6/miniconda3/envs/vijay-code-edit/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1890, in generate\n",
      "    raise ValueError(\"assisted generate is not supported with `static_cache`\")\n",
      "ValueError: assisted generate is not supported with `static_cache`\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Path\n",
    "from pydantic import BaseModel\n",
    "from typing import Annotated, Union\n",
    "import uvicorn\n",
    "import asyncio\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from transformers import TextStreamer\n",
    "import time\n",
    "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class EditRequest(BaseModel):\n",
    "    file_content: str\n",
    "    query: str\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.post(\"/edit_request\")\n",
    "async def edit_request(edit_request: EditRequest):\n",
    "    start_time = time.time()\n",
    "    # prompt = \"Code:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:\\n\".format(code_text=code_text, question=question)\n",
    "    shot = \"\"\"## Code Before:\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "## Instruction:\n",
    "Add a \"sub\" function that subtracts two numbers. Also write docstrings for both functions and change a,b to x,y.\n",
    "## Code After:\n",
    "def add(x, y):\n",
    "    \\\"\\\"\\\"Adds two numbers.\\\"\\\"\\\"\n",
    "    return x + y\n",
    "\n",
    "def sub(x, y):\n",
    "    \\\"\\\"\\\"Subtracts two numbers.\\\"\\\"\\\"\n",
    "    return x - y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = shot + \"\\n\" + f\"\"\"## Code Before:\n",
    "{edit_request.file_content}\n",
    "## Instruction:\n",
    "{edit_request.query}\n",
    "## Code After:\n",
    "\"\"\"\n",
    "    # prompt = f\"<commit_before>\\n{edit_request.file_content}\\n<commit_msg>\\n{edit_request.query}\\n<commit_after>\\n\"\n",
    "    \n",
    "    code_inputs = tokenizer(edit_request.file_content, return_tensors=\"pt\").input_ids[0].tolist()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # inputs = tokenizer.apply_chat_template(\n",
    "    #     [\n",
    "    #         {\n",
    "    #             \"role\": \"user\",\n",
    "    #             \"content\": prompt\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"role\": \"assistant\",\n",
    "    #             \"content\": \"```\\n\"\n",
    "    #         }\n",
    "    #     ],\n",
    "    #     tokenize=True,\n",
    "    #     add_generation_prompt=True,\n",
    "    #     return_tensors=\"pt\"\n",
    "    # ).to(model.device)\n",
    "    \n",
    "    # Move all tensor values in the inputs to GPU\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "    # num_max_gen_tokens = inputs.shape[-1] + 300\n",
    "    num_max_gen_tokens = inputs.input_ids.shape[-1] + 300\n",
    "\n",
    "    # diff_candidate_generator = DiffPromptLookupCandidateGenerator(\n",
    "    #     inputs.input_ids, \n",
    "    #     code_inputs\n",
    "    # )\n",
    "    \n",
    "    # model._get_candidate_generator = (_get_default_candidate_generator_generator(diff_candidate_generator)).__get__(model, type(model))\n",
    "    \n",
    "    two_layer_candidate_generator = CodeTwoLayerLookupCandidateGenerator(\n",
    "        tokenizer,\n",
    "        inputs.input_ids.shape[-1],\n",
    "        draft_model,\n",
    "        inputs.input_ids,\n",
    "        code_inputs,\n",
    "        ngram_size=5,\n",
    "        num_pred_tokens=50\n",
    "    )\n",
    "    model._get_candidate_generator = (_get_default_candidate_generator_generator(two_layer_candidate_generator)).__get__(model, type(model))\n",
    "    with sdpa_kernel(SDPBackend.MATH):\n",
    "        test_out = model.generate(\n",
    "            inputs=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            prompt_lookup_num_tokens=1,\n",
    "            max_new_tokens=num_max_gen_tokens,\n",
    "            stopping_criteria=[StopStringCriteria(tokenizer, [\"\\n\\n\\n\\n\\n\"])],\n",
    "            use_cache=True,\n",
    "            streamer=TextStreamer(tokenizer)\n",
    "        )\n",
    "        \n",
    "    text = tokenizer.batch_decode(test_out[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0]\n",
    "    # text = tokenizer.batch_decode(test_out, skip_special_tokens=True)[0]\n",
    "    \n",
    "    unified_diff = \"\\n\".join(difflib.unified_diff(edit_request.file_content.splitlines(), text.splitlines(), n=3))\n",
    "    search_replace_changes = parse_diff(unified_diff)\n",
    "    # print(\"Search replace changes: \", search_replace_changes)\n",
    "    \n",
    "    fixed_file = edit_request.file_content\n",
    "    for sr in search_replace_changes:\n",
    "        if len(sr.search_block.strip()) == 0:\n",
    "            continue\n",
    "        if sr.search_block.strip() == sr.replace_block.strip():\n",
    "            continue\n",
    "        print(\"SEARCH\\n\", sr.search_block)\n",
    "        sr.search_block = find_best_match(sr.search_block, edit_request.file_content).block\n",
    "        print(\"FOUND BLOCK\\n\", sr.search_block)\n",
    "        print(\"REPLACE\\n\", sr.replace_block.rstrip())\n",
    "        fixed_file = fixed_file.replace(sr.search_block, f\"\"\"<<<<<<< SEARCH\n",
    "{sr.search_block}\n",
    "=======\n",
    "{sr.replace_block.rstrip()}\n",
    ">>>>>>> REPLACE\"\"\")\n",
    "\n",
    "    return {\"text\": fixed_file, \"tokens_input\": inputs.input_ids.shape[-1], \"tokens_generated\": test_out.shape[-1] - inputs.input_ids.shape[-1], \"time\": time.time() - start_time}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app)\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67534b8-7db7-49f1-8711-309d978bcaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
