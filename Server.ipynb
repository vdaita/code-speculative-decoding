{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e94e4e-8ca0-462a-9f41-95a2f41a944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers # requires transformers==4.35.2\n",
    "device = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1668d6c1-d608-4f91-8741-b33693e83d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e27a50-63e6-45c1-88c2-e43d2fa94bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AwqConfig\n",
    "\n",
    "# target_quantization_config = AwqConfig(\n",
    "#     bits=4,\n",
    "#     fuse_max_seq_len=65536,\n",
    "#     modules_to_fuse={\n",
    "#         \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "#         \"layernorm\": [\"ln1\", \"ln2\", \"norm\"],\n",
    "#         \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#         \"use_alibi\": False,\n",
    "#         \"num_attention_heads\": 32,\n",
    "#         \"num_key_value_heads\": 4,\n",
    "#         \"hidden_size\": 4096\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# model_name = \"Qwen/CodeQwen1.5-7B-Chat-AWQ\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.float16, use_flash_attention_2=True, quantization_config=target_quantization_config)#, load_in_4bit=True)#  , use_flash_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97415ab-2c4b-4356-a6e8-3e6c2bdfe6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft_quantization_config = AwqConfig(\n",
    "#     bits=4,\n",
    "#     fuse_max_seq_len=32768,\n",
    "#     modules_to_fuse={\n",
    "#         \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "#         \"layernorm\": [\"ln1\", \"ln2\", \"norm\"],\n",
    "#         \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#         \"use_alibi\": False,\n",
    "#         \"num_attention_heads\": 16,\n",
    "#         \"num_key_value_heads\": 16,\n",
    "#         \"hidden_size\": 1024\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# draft_model_name = \"Qwen/Qwen1.5-0.5B-Chat-AWQ\"\n",
    "# draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, trust_remote_code=True, device_map=\"auto\", use_flash_attention_2=True, quantization_config=draft_quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab011d54-7e30-4fc1-a70c-f92d81737801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cbcb04fe6c404b8c6ce2e1fe27dbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "model_name = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "draft_model_name = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, trust_remote_code=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be64b3a4-aff3-4330-8e09-e2ff43823861",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd3b3c4-7eb8-47f2-9bec-87d0a12bf98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1112]\n",
      "[198]\n",
      "[565]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"...\"))\n",
    "print(tokenizer.encode(\"\"\"\n",
    "\"\"\"))\n",
    "print(tokenizer.encode(\"##\"))\n",
    "\n",
    "newline_token = tokenizer.encode(\"\"\"\n",
    "\"\"\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640f2d62-7976-4d26-beb1-08aa01043736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n",
    "from transformers.models.auto import (\n",
    "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
    "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
    ")\n",
    "from transformers.utils import ExplicitEnum, ModelOutput, is_accelerate_available, logging\n",
    "from transformers.generation.beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from transformers.generation.logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    EncoderRepetitionPenaltyLogitsProcessor,\n",
    "    EpsilonLogitsWarper,\n",
    "    EtaLogitsWarper,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    MinNewTokensLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SequenceBiasLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    "    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "\n",
    "from transformers.generation.utils import _crop_past_key_values\n",
    "import difflib\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using greedy search.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
    "            if all batches finished early due to the `eos_token_id`.\n",
    "        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
    "            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
    "        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88cde95-4062-4a4c-8ef2-146c693f3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            # if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "            #     return input_ids[0, start_idx:end_idx]\n",
    "            if start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:min(end_idx, input_length)]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([100], dtype=torch.long, device=input_ids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5466bcc-865b-4333-9cae-4d3fa4afd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens_diff(input_ids, code_ids, orig_input_len=0, ngram_size=3, num_pred_tokens=10):\n",
    "    # start_time = time.perf_counter()\n",
    "    input_length = input_ids.size(1)\n",
    "    code_length = len(code_ids)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if ngram_size <= 0 or ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    sm = difflib.SequenceMatcher(None, code_ids, input_ids[0, orig_input_len:].tolist())\n",
    "    \n",
    "    deleted = added = changed = same = last_deleted = 0\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            changed += i2 - i1\n",
    "        elif tag == 'delete':\n",
    "            deleted += i2 - i1\n",
    "            last_deleted = i2 - i1\n",
    "        elif tag == 'insert':\n",
    "            added += j2 - j1\n",
    "        elif tag == 'equal':\n",
    "            same += i2 - i1\n",
    "    \n",
    "    approx_tokens_original = changed + deleted + same - last_deleted\n",
    "\n",
    "    lookback_start = max(input_length - ngram_size, orig_input_len)\n",
    "    search_ngram = input_ids[0, lookback_start:].tolist()\n",
    "\n",
    "    for ngram_start in range(max(0, approx_tokens_original - ngram_size), len(code_ids)):\n",
    "        # if there is a match, return the entire rest of the tokens.\n",
    "        if ngram_start + len(search_ngram) >= len(code_ids):\n",
    "            break\n",
    "        if search_ngram == code_ids[ngram_start:ngram_start + len(search_ngram)]:\n",
    "            return torch.tensor(code_ids[ngram_start + len(search_ngram):max(ngram_start + len(search_ngram) + num_pred_tokens, len(code_ids))], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "    # If no match is found, return what the answer would be otherwise\n",
    "    # print(\"Diff searching took: \", time.perf_counter() - start_time)\n",
    "    return find_candidate_pred_tokens(input_ids, ngram_size, num_pred_tokens)\n",
    "    # return torch.tensor([], dtype=torch.long, device=input_ids.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7bc8977-4805-48d6-b1af-d0be832876ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
    "UNDERLINE = \"\\x1b[4m\"\n",
    "RESET = \"\\x1b[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bca616bf-a9b7-4fe4-9276-86a311b5472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.candidate_generator import CandidateGenerator, _crop_past_key_values\n",
    "from transformers.generation.stopping_criteria import StoppingCriteria\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from typing import Tuple\n",
    "\n",
    "class DiffPromptLookupCandidateGenerator(CandidateGenerator):\n",
    "    def __init__(self, input_ids, code_ids, ngram_size=3, num_pred_tokens=10):\n",
    "        self.code_ids = code_ids\n",
    "        self.orig_input_len = input_ids.shape[-1]\n",
    "        self.ngram_size = ngram_size\n",
    "        self.num_pred_tokens = num_pred_tokens\n",
    "    \n",
    "    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n",
    "        # print(\"Getting candidates\")\n",
    "        return torch.cat(\n",
    "            (\n",
    "                input_ids,\n",
    "                find_candidate_pred_tokens_diff(input_ids, self.code_ids, self.orig_input_len, self.ngram_size, self.num_pred_tokens).unsqueeze(0)\n",
    "            ),\n",
    "            dim=-1\n",
    "        ), None\n",
    "    \n",
    "    def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int): # Maybe use the number of matches/scores to have a threshold\n",
    "        pass \n",
    "\n",
    "class NumRunsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, max_num_runs=4):\n",
    "        self.max_num_runs = 4\n",
    "        self.num_runs = 0\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
    "        self.num_runs += 1\n",
    "        return self.num_runs >= self.max_num_runs\n",
    "\n",
    "def _get_default_candidate_generator_generator(generator: CandidateGenerator):\n",
    "    def _get_candidate_generator(self, **kwargs):\n",
    "        return generator\n",
    "    return _get_candidate_generator\n",
    "\n",
    "class TwoLayerLookupCandidateGenerator(CandidateGenerator):\n",
    "    def __init__(self, draft_model, input_ids, code_ids, num_runs=4, **diff_prompt_args):\n",
    "        self.draft_model = draft_model\n",
    "        self.input_ids = input_ids\n",
    "        self.code_ids = code_ids\n",
    "        self.candidate_generator = DiffPromptLookupCandidateGenerator(\n",
    "            self.input_ids, \n",
    "            self.code_ids,\n",
    "            **diff_prompt_args\n",
    "        )\n",
    "        \n",
    "        self.past_keys_values = None\n",
    "        self.num_runs = num_runs\n",
    "\n",
    "        self.draft_model._get_candidate_generator = (_get_default_candidate_generator_generator(self.candidate_generator)).__get__(self.draft_model, type(self.draft_model))\n",
    "\n",
    "    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n",
    "        if self.past_keys_values:\n",
    "            self.past_keys_values = _crop_past_keys_values(self.draft_model, self.past_keys_values, input_ids.shape[-1] - 1)\n",
    "\n",
    "        starting_input_length = input_ids.shape[-1]\n",
    "        \n",
    "        generation = self.draft_model.generate(\n",
    "            inputs=input_ids,\n",
    "            prompt_lookup_num_tokens=1,\n",
    "            max_new_tokens=1000,\n",
    "            stopping_criteria=[NumRunsStoppingCriteria(self.num_runs)],\n",
    "            past_key_values=self.past_keys_values,\n",
    "            use_cache=True,\n",
    "            # output_logits=True,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "        return generation.sequences, None\n",
    "\n",
    "    def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int): # Maybe use the number of matches/scores to have a threshold\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1d692b-10d3-487e-a782-f8dd57602ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Hunk:\n",
    "    filepath: str\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class SearchReplaceChange:\n",
    "    filepath: str\n",
    "    search_block: str\n",
    "    replace_block: str\n",
    "\n",
    "def find_hunks(diff_string):\n",
    "    hunks = []\n",
    "    current_filename = \"\"\n",
    "    current_lines = \"\"\n",
    "    for line in diff_string.splitlines():\n",
    "        if line.startswith(\"---\"):\n",
    "            continue\n",
    "        elif line.lstrip().startswith(\"+++\"):\n",
    "            if len(current_filename) > 0:\n",
    "                hunks.append(Hunk(current_filename, current_lines))\n",
    "            current_filename = line[3:]\n",
    "            current_lines = \"\"\n",
    "        elif line.lstrip().startswith(\"@@\"):\n",
    "            if len(current_filename) > 0:\n",
    "                hunks.append(Hunk(current_filename, current_lines))\n",
    "            current_lines = \"\"\n",
    "        else:\n",
    "            current_lines += line\n",
    "            current_lines += \"\\n\"\n",
    "    hunks.append(Hunk(current_filename, current_lines))\n",
    "    return hunks\n",
    "\n",
    "def parse_diff(diff_string):\n",
    "    hunks = find_hunks(diff_string)\n",
    "    search_replace_blocks = []\n",
    "\n",
    "    for hunk in hunks:\n",
    "        filepath = hunk.filepath\n",
    "        text = hunk.text\n",
    "\n",
    "        search_block = \"\"\n",
    "        replace_block = \"\"\n",
    "\n",
    "        for line in text.splitlines():\n",
    "            if line.startswith(\"-\"):\n",
    "                search_block += \" \" + line[1:] + \"\\n\"\n",
    "            elif line.startswith(\"+\"):\n",
    "                replace_block += \" \" + line[1:] + \"\\n\"\n",
    "            else:\n",
    "                search_block += line + \"\\n\"\n",
    "                replace_block += line + \"\\n\"\n",
    "\n",
    "        search_replace_blocks.append(\n",
    "            SearchReplaceChange(filepath, search_block, replace_block)\n",
    "        )\n",
    "        filepath = \"\"\n",
    "        search_block = \"\"\n",
    "        replace_block = \"\"\n",
    "\n",
    "    search_replace_blocks.append(\n",
    "        SearchReplaceChange(filepath, search_block, replace_block)\n",
    "    )\n",
    "\n",
    "    return search_replace_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3598b1e-8a2b-4aba-987c-898ee88ce477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [930110]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "# Code before:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Calculate the average\n",
      "average_throughput = np.mean(tokens_per_sec_arr)\n",
      "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
      "\n",
      "# Plotting the histogram\n",
      "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
      "plt.title('Histogram of Throughput Values')\n",
      "plt.xlabel('Tokens per Second')\n",
      "plt.ylabel('Frequency')\n",
      "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
      "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      "plt.show()\n",
      "\n",
      "# Requested change:\n",
      "Can you please change x axis to start from 0\n",
      "# Code After:\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sure, here's the updated code with the x-axis starting from 0:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Calculate the average\n",
      "average_throughput = np.mean(tokens_per_sec_arr)\n",
      "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
      "\n",
      "# Reset x-axis to start from 0\n",
      "plt.xlim(0, max(tokens_per_sec_arr) + 1)\n",
      "\n",
      "# Plotting the histogram\n",
      "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
      "plt.title('Histogram of Throughput Values')\n",
      "plt.xlabel('Tokens per Second (Starting from 0)')\n",
      "plt.ylabel('Frequency')\n",
      "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
      "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "In this code, I added `xlim(0, max(tokens_per_sec_arr) + 1)` to set the x-axis limits to start from 0 and include the maximum value in the array. This ensures that the histogram starts at 0 and covers the entire range of the data.<|im_end|>\n",
      "<|endoftext|>\n",
      "Search:  \n",
      " import numpy as np\n",
      " import matplotlib.pyplot as plt\n",
      " \n",
      " # Calculate the average\n",
      " average_throughput = np.mean(tokens_per_sec_arr)\n",
      " print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
      " \n",
      " # Plotting the histogram\n",
      " plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
      "\n",
      "Replace:  \n",
      " hput: {average_throughput} tokens/sec\")\n",
      " \n",
      " # Plotting the histogram\n",
      " plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
      "\n",
      "Search:  \n",
      " plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
      " plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      " plt.show()\n",
      "\n",
      "Replace:  \n",
      " plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
      " plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      " plt.show()\n",
      " \n",
      " # Requested change:\n",
      " Can you please change x axis to start from 0\n",
      " # Code After:\n",
      " \n",
      " assistant\n",
      " Sure, here's the updated code with the x-axis starting from 0:\n",
      " \n",
      " ```python\n",
      " import numpy as np\n",
      " import matplotlib.pyplot as plt\n",
      " \n",
      " # Calculate the average\n",
      " average_throughput = np.mean(tokens_per_sec_arr)\n",
      " print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
      " \n",
      " # Reset x-axis to start from 0\n",
      " plt.xlim(0, max(tokens_per_sec_arr) + 1)\n",
      " \n",
      " # Plotting the histogram\n",
      " plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
      " plt.title('Histogram of Throughput Values')\n",
      " plt.xlabel('Tokens per Second (Starting from 0)')\n",
      " plt.ylabel('Frequency')\n",
      " plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
      " plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      " plt.show()\n",
      " ```\n",
      " \n",
      " In this code, I added `xlim(0, max(tokens_per_sec_arr) + 1)` to set the x-axis limits to start from 0 and include the maximum value in the array. This ensures that the histogram starts at 0 and covers the entire range of the data.\n",
      "\n",
      "INFO:     127.0.0.1:36510 - \"POST /edit_request HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Path\n",
    "from pydantic import BaseModel\n",
    "from typing import Annotated, Union\n",
    "import uvicorn\n",
    "import asyncio\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from transformers import TextStreamer\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class EditRequest(BaseModel):\n",
    "    file_content: str\n",
    "    query: str\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.post(\"/edit_request\")\n",
    "async def edit_request(edit_request: EditRequest):\n",
    "    # prompt = \"Code:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:\\n\".format(code_text=code_text, question=question)\n",
    "    prompt = f\"# Code before:\\n{edit_request.file_content}\\n# Requested change:\\n{edit_request.query}\\n# Code After:\\n\"\n",
    "    \n",
    "    code_inputs = tokenizer(edit_request.file_content, return_tensors=\"pt\").input_ids[0].tolist()\n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Move all tensor values in the inputs to GPU\n",
    "    # for key in inputs:\n",
    "    #     inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "    num_max_gen_tokens = inputs.shape[-1] + 300\n",
    "\n",
    "    # diff_candidate_generator = DiffPromptLookupCandidateGenerator(\n",
    "    #     inputs.input_ids, \n",
    "    #     code_inputs\n",
    "    # )\n",
    "    \n",
    "    # draft_model._get_candidate_generator = (_get_default_candidate_generator_generator(diff_candidate_generator)).__get__(draft_model, type(draft_model))\n",
    "    \n",
    "\n",
    "    two_layer_candidate_generator = TwoLayerLookupCandidateGenerator(\n",
    "        draft_model,\n",
    "        inputs,\n",
    "        code_inputs,\n",
    "        ngram_size=3,\n",
    "        num_pred_tokens=10\n",
    "    )\n",
    "    model._get_candidate_generator = (_get_default_candidate_generator_generator(two_layer_candidate_generator)).__get__(model, type(model))\n",
    "\n",
    "    test_out = model.generate(\n",
    "        inputs=inputs,\n",
    "        prompt_lookup_num_tokens=2,\n",
    "        max_new_tokens=num_max_gen_tokens,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    \n",
    "    text = tokenizer.batch_decode(test_out, skip_special_tokens=True)[0][inputs.shape[-1]:]\n",
    "\n",
    "    unified_diff = \"\\n\".join(difflib.unified_diff(edit_request.file_content.splitlines(), text.splitlines(), n=3))\n",
    "    search_replace_changes = parse_diff(unified_diff)\n",
    "    # print(\"Search replace changes: \", search_replace_changes)\n",
    "    \n",
    "    fixed_file = edit_request.file_content\n",
    "    for sr in search_replace_changes:\n",
    "        if len(sr.search_block.strip()) == 0:\n",
    "            continue\n",
    "        print(\"Search: \", sr.search_block)\n",
    "        print(\"Replace: \", sr.replace_block)\n",
    "        fixed_file = fixed_file.replace(sr.search_block, f\"\"\"<<<<<<< SEARCH\n",
    "{sr.search_block}\n",
    "=======\n",
    "{sr.replace_block}\n",
    ">>>>>>> REPLACE\"\"\")\n",
    "\n",
    "    return {\"text\": fixed_file}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app)\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12263882-6012-47a0-9865-28686a7dd0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
